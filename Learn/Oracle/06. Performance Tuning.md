## Chapter 1: Selectivity and Cardinality

Performance tuning is one area where most Junior DBAs face those tough-to-break problems. The kind of problems which require deep understanding of the concepts before you can even point what is going wrong in the system. We will be going through the basics knowledge in the Performance Tuning area in series of posts.

### Selectivity

**Definition**: It represents the fraction of rows filtered by an operation, so you can say it is a measure of uniqueness.

#### Key Characteristics:
- **Range**: Its value is between 0 and 1
- **Calculation**: selectivity = (rows returned after filter) / (total rows before filter)

#### Examples:

**Example 1 - Good Selectivity:**
- Query returned 100 rows initially
- After applying filter (WHERE condition), final result is 10 rows
- Selectivity = 10/100 = 0.1 (or 10%)
- This is **GOOD selectivity** because the filter significantly reduced the result set

**Example 2 - Bad Selectivity:**
- Query returned 200 rows initially  
- After applying filter, final result is still 200 rows
- Selectivity = 200/200 = 1.0 (or 100%)
- This is **BAD selectivity** because the filter didn't reduce the result set at all

#### Selectivity Classifications:

**GOOD Selectivity:**
- A column is highly selective if a SQL returns a small number of duplicate rows
- Means the filter is effective at narrowing down results
- Results in better performance

**BAD Selectivity:**
- A column is least selective if a SQL returns all or large number of rows
- Means the filter is not effective
- Results in poor performance

#### Important Notes:
- When you run `SELECT * FROM EMP` without any filters, selectivity will be automatically 1 as all rows will be returned
- **Adding a composite Index** is the best way to make BAD selectivity become GOOD selectivity
- Using more than one column makes the Index more unique which improves Index selectivity

### Cardinality

**Definition**: The number of rows returned by an operation is the cardinality.

#### Relationship Formula:
```
cardinality = selectivity × number of input rows
```

#### Practical Example:
- Query initially retrieved 200 records from database
- After applying filters, final number of rows is 50
- **Selectivity** = 50/200 = 0.25 (25%)
- **Cardinality** = 50 (or 200 × 0.25 = 50)

#### Impact on Performance:
Sometimes the Oracle optimizer is not able to predict the number of rows that a given operator will return due to reasons like:
- Missing table statistics
- Outdated statistics
- Complex predicates

This can prevent Oracle from estimating the cost of a query plan correctly, which can lead to:
- Selection of suboptimal execution plans
- Cardinality estimation errors
- Slow running queries

### Detailed Examples

#### Example 1: Query Without Filter
```sql
SELECT MAX(EMP_NUMBER) FROM EMP;
```
**Scenario**: Table EMP has 10 records total

**Analysis**:
- **Selectivity** = number of rows accessed / total number of rows = 10/10 = 1.0
- **Interpretation**: 100% of the rows were accessed
- **Cardinality** = number of rows accessed = 10

#### Example 2: Query With Filter  
```sql
SELECT MAX(EMP_NUMBER) FROM EMP WHERE LAST_NAME = 'SMITH';
```
**Scenario**: Only 4 employees have LAST_NAME as 'SMITH' out of 10 total records

**Analysis**:
- **Selectivity** = number of rows accessed / total number of rows = 4/10 = 0.4
- **Interpretation**: 40% of the rows were accessed
- **Cardinality** = number of rows accessed = 4

This demonstrates how adding a selective filter condition improves selectivity from 1.0 to 0.4, making the query more efficient.

---

## Chapter 2: Parsing

From performance tuning perspectives, it is very important to understand the concept of parsing. Parsing is the primary phase in SQL execution, followed by other stages: Execute and Fetch.

### Parsing Basics

Whenever a SQL statement is executed, the Oracle Engine performs the following actions:

1. **Validate the Syntax** - Check if the SQL statement is syntactically correct
2. **Validate the Objects** - Verify that all objects referenced in the statement exist
3. **Check Privileges** - Ensure the user has necessary privileges to execute the statement
4. **Search Shared Pool** - Verify if the statement is already available in the shared pool by:
   - Oracle engine calculates the hash value for the SQL statement
   - Looks in the shared pool for matching hash
5. **Allocate Memory** - If statement is not present, allocate shared memory and create a cursor in shared pool
6. **Generate Execution Plan** - Create the optimal execution plan for the statement

### Types of Parses

#### Hard Parse

**Definition**: A hard parse occurs when the statement is not available in shared memory or this is a brand new statement that the user is trying to execute.

**When Hard Parse Occurs**:
- Statement has never been executed before
- Statement was aged out of shared pool due to memory pressure
- Statement text doesn't exactly match existing statements (even case sensitivity matters)

**Process**: All parsing steps (1-6 above) need to be completed

**Impact**: 
- Requires extra system resources
- CPU-intensive operation
- Also known as **'Library Cache Miss'**

#### Soft Parse

**Definition**: A soft parse occurs when the statement was executed earlier, was already parsed, and is available in memory.

**Process**: Oracle only needs to perform steps 1-3 (syntax validation, object validation, privilege check) since the remaining tasks were already completed earlier.

**Benefits**:
- Much faster than hard parse
- Minimal resource consumption
- Also known as **'Library Cache Hit'**
- Follows the principle: "work hard once and reap benefits multiple times"

### Why Hard Parses Should Be Avoided

There are two key reasons why hard parses should be kept to the bare minimum required:

#### 1. CPU Intensive Operations
- **Generation of an execution plan is a very CPU-intensive operation**
- Each hard parse consumes significant CPU resources
- High hard parse rates can lead to CPU bottlenecks

#### 2. Memory Serialization Issues
- **Memory in the shared pool is limited**
- **Memory operations are serialized** - they must happen one at a time
- Memory operations use **shared pool latches** and **library cache latches**
- When many hard parses happen simultaneously:
  - Other processes must wait in queue to get the shared pool latch
  - This creates contention and reduces overall system performance
  - Impacts both shared pool latch and library cache latch availability

### Performance Implications

#### Hard Parse Impact:
- High CPU consumption
- Memory contention
- Increased response times
- Reduced throughput
- Latch waits

#### Soft Parse Benefits:
- Low CPU consumption  
- Reduced memory operations
- Faster response times
- Higher throughput
- Better scalability

### Best Practices

1. **Use Bind Variables** - Promotes statement reuse and soft parsing
2. **Consistent SQL Text** - Ensure identical statements have identical text (case, spacing, etc.)
3. **Adequate Shared Pool Size** - Prevent aging out of frequently used statements
4. **Monitor Parse Ratios** - Track hard vs soft parse ratios
5. **Application Design** - Design applications to reuse SQL statements

### Monitoring Parsing

Key metrics to monitor:
- Hard parse rate
- Soft parse rate  
- Parse time CPU vs total CPU
- Library cache hit ratio
- Shared pool latch contention

---

