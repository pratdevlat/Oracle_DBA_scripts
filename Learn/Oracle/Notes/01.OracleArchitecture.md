# Overview

## Database

A collection of operating system files or disks. Oracle 12c provides three different types of databases:
- **Single-tenant database**: Completely self-contained with a full set of data files, control files, redo log files, parameter files, etc. Contains all metadata, data, code, and all application-related metadata, data, and code. All Oracle databases prior to 12c are of this type.
- **Container or root database (CDB)**: Contains a full set of data files, control files, redo log files, and parameter files, but only used to store Oracle's own metadata, internal data, and internal code. It does not store application data or code, only Oracle-specific entities. This database is entirely self-contained and can be mounted and opened independently.
- **Pluggable database (PDB)**: Contains only data files and is not fully self-contained. It must be attached (plugged) to a container database (CDB) to be opened for read and write operations. This database holds only application metadata, objects, data, and code. It relies on the files (control files, redo logs, parameter files) from the CDB.

## Instance

An Oracle instance consists of a set of Oracle background processes/threads and a shared memory region, used by threads/processes running on the same computer. Oracle stores and maintains volatile, non-persistent content here (some of which may be flushed to disk). Importantly, a database instance can exist without disk storage.

Relationships between instances and databases:
- A single-tenant or container database can be mounted and opened by multiple instances, but an instance can only mount and open one database at any given time. An instance can mount and open at most one database during its entire lifecycle.
- For a pluggable database (PDB), it can only be associated with one container database (CDB) at any given time, and thus with one instance. After an instance opens and mounts a CDB, its contained PDBs will also use this instance. An instance can concurrently access multiple PDBs (up to approximately 250), meaning each instance can serve multiple PDBs simultaneously, but it can only open one CDB or single-tenant database at a time.

### Dedicated Server

The default mode of connection is the **dedicated server** mode. In this setup, Oracle creates a new process or thread for each user connection. Each user connecting to the database gets their own dedicated server process that receives and executes SQL over a network channel.

### Shared Server

Oracle also allows connections via **shared server**. In this mode, the database doesn't create new threads or processes for each user connection.

Oracle uses a pool of "shared processes" to serve multiple users. Shared server is effectively a connection pooling mechanism, sharing processes among sessions. Oracle utilizes one or more **dispatcher processes** to handle client requests. Client processes communicate over a network with a dispatcher, which places requests into a queue in the SGA. The first free shared server picks up the request and processes it. Upon completion, the shared server places responses back into the dispatcher's response queue. Dispatchers monitor this queue and return results to the client.

### Pluggable Database

A pluggable database (PDB) under the multitenant architecture comprises a set of non-self-contained data files, containing only application data and metadata. Oracle-specific data isn't stored here but resides in the container database (CDB). To use or query a PDB, it must be "plugged" into a CDB. The CDB contains only Oracle-specific necessary runtime data and metadata. PDB stores remaining data and metadata.

Oracle designed PDBs and multitenant architecture primarily to:
- Efficiently reduce resource usage by multiple databases/applications on a single host.
- Reduce DBA maintenance efforts for multiple databases/applications on a single host.

---
# Oracle Database Architecture: SGA, PGA, and Background Processes (In-Depth)

---

## 1. System Global Area (SGA)

The **SGA** is shared memory allocated at instance startup, containing data and control information used by all server and background processes.

### Components:

* **Database Buffer Cache**: Caches data blocks read from disk, optimizing I/O. Managed by **DBWn** background processes.
* **Redo Log Buffer**: Buffers redo entries before writing to online redo logs via **LGWR**.
* **Shared Pool**:

  * **Library Cache**: Caches parsed SQL and PL/SQL code, enables soft parsing.
  * **Data Dictionary Cache**: Holds metadata about database objects.
* **Large Pool**: Supports large allocations (RMAN, parallel execution, UGA in shared server).
* **Java Pool**: Stores Java code and data.
* **Streams Pool**: Supports Oracle Streams replication.

### Memory Management:

* **Automatic Memory Management (AMM)**: Uses `MEMORY_TARGET` to manage SGA + PGA.
* **Automatic Shared Memory Management (ASMM)**: Uses `SGA_TARGET`.
* **Manual**: Explicitly set `DB_CACHE_SIZE`, `SHARED_POOL_SIZE`, etc.

### Monitoring:

```sql
SELECT pool, name, bytes
FROM v$sgastat
WHERE pool IN ('shared pool', 'large pool', 'java pool');

SELECT pool, ROUND(bytes/1024/1024,0) AS free_mb
FROM v$sgastat
WHERE name LIKE '%free memory%';
```

### Additional Tips:

* Use **hugepages** on Linux for better performance.


## 2. Program Global Area (PGA)

The **PGA** is process-private memory allocated per session or background process.

### Components:

* **Session Memory**: Login/session state.
* **Private SQL Area**: Stores parsed SQL, bind variables.
* **SQL Work Areas**: For sorting, hash joins, bitmap merges.

### Key Parameters:

* `PGA_AGGREGATE_TARGET`: Suggests total PGA allocation.

### Monitoring:

```sql
SHOW PARAMETER pga_aggregate_target;

SELECT *
FROM v$pga_target_advice
ORDER BY pga_target_for_estimate;
```

### Best Practices:

* Keep PGA hit ratios >60% to avoid disk spills.
* DSS workloads: higher PGA, OLTP: lower.

---

## 3. Background Processes

Background processes handle I/O, memory management, recovery, and system tasks.

### Mandatory Processes:

| Process  | Description                                           |
| -------- | ----------------------------------------------------- |
| **PMON** | Cleans failed sessions, re-registers with listener    |
| **SMON** | Instance recovery, temp segment cleanup               |
| **DBWn** | Writes dirty buffers to disk                          |
| **LGWR** | Writes redo log buffer to redo logs                   |
| **CKPT** | Signals checkpoints, updates control/datafile headers |
| **ARCn** | Archives redo logs in ARCHIVELOG mode                 |
| **RECO** | Resolves distributed transactions                     |
| **LREG** | Registers services with listener                      |

### Optional Processes:

* **CJQ0**: Job queue coordinator.
* **MMON, MMNL**: Memory management monitoring.
* **FBDA**: Fast incremental backup.

### Monitoring:

```sql
SELECT p.spid, p.program
FROM v$process p
WHERE p.pname IS NOT NULL
ORDER BY p.spid;
```

---

## 4. Instance Lifecycle

* **Startup**:

  * Reads init parameters.
  * Allocates SGA.
  * Starts background processes.
  * Opens control files, datafiles, redo logs.
* **Shutdown**:

  * Flushes buffers.
  * Closes files.
  * Terminates background processes.

---

## 5. Additional Views

| View                  | Description                   |
| --------------------- | ----------------------------- |
| `v$sgastat`           | SGA allocation details        |
| `v$pga_target_advice` | PGA tuning advice             |
| `v$process`           | OS-level background processes |
| `v$session`           | Session details               |
| `v$sysstat`           | Instance statistics           |

---

## 6. Tips for Real-World Usage

* **Use AWR Reports**: Identify SGA/PGA usage trends, top SQL, wait events.
* **Consider RAC**: Adds global cache services (GCS), global enqueue services (GES).
* **Monitor Wait Events**: High CPU or waits may indicate SGA/PGA tuning needed.

---





# Memory Structure

Oracle has three main memory structures:
- **System Global Area (SGA)**: This is a large shared memory segment that almost all Oracle processes need to access.
- **Process Global Area (PGA)**: This is a private memory area for a process or thread, which other processes/threads cannot access.
- **User Global Area (UGA)**: This memory area is associated with a specific session. It may be allocated in either the SGA or the PGA, depending on whether the database connection is using a shared server or a dedicated server. If a shared server is used, UGA is allocated in SGA; if a dedicated server is used, UGA is allocated in PGA.

## Process Global Area and User Global Area

PGA (Process Global Area) is a memory segment specific to a process. In other words, it is a private memory allocated to an operating system process or thread, preventing other processes or threads from accessing it. PGA is usually allocated using `malloc()` or `memmap()` and can dynamically expand or shrink during execution. PGA is never allocated in Oracle’s SGA; it is always allocated by the process or thread itself.

UGA (User Global Area) essentially represents the state of your session. It is a memory segment that remains accessible for your session throughout. The allocation of UGA depends entirely on how you connect to Oracle.

Since Oracle 9iR1, there have been two ways to manage non-UGA memory in PGA:
- **Manual PGA Memory Management**: You specify how much memory a process can use for sorting or hashing.
- **Automatic PGA Memory Management**: You tell Oracle how much total PGA memory it can try to use system-wide.

Since Oracle 11gR1, automatic PGA memory management can be implemented using two techniques:
- By setting the `PGA_AGGREGATE_TARGET` initialization parameter, you tell Oracle how much memory PGA can try to use instance-wide.
- By setting the `MEMORY_TARGET` initialization parameter, you tell Oracle the total memory (SGA and PGA combined) the instance should be allowed to use. The database determines the appropriate PGA size itself based on this parameter.

The amount of PGA memory allocated per process is typically determined based on total available memory and the number of competing processes. As workload increases, the memory allocated to individual work areas decreases. The database tries to ensure total PGA memory usage does not exceed the `PGA_AGGREGATE_TARGET`, but if necessary, it will exceed the limit to maintain database operations.

## System Global Area

Every Oracle database instance has a large memory structure called the **System Global Area (SGA)**. This is a large shared memory segment accessed by all Oracle processes.

SGA consists of several pools, including:
- **Java Pool**: A fixed-size memory allocated for the Java Virtual Machine running within the database. It can be dynamically resized.
- **Large Pool**: Used for session memory (UGA) in shared server connections, message buffers in parallel execution, and disk I/O buffers during RMAN backups. It can be dynamically resized.
- **Shared Pool**: Contains shared cursors, stored procedures, state objects, dictionary cache, and other shared data. It can be dynamically resized.
- **Streams Pool**: A memory pool dedicated to data transfer and sharing mechanisms. It can be dynamically resized.
- **"Null" Pool**: This unnamed pool includes block buffers (for caching database blocks), redo log buffers, and the fixed SGA area.

---

# Oracle Processes

Each process in Oracle performs a specific task (or a set of tasks), allocating memory (PGA) for itself to complete its operations. An Oracle instance primarily consists of three types of processes:
- **Server processes:** These processes complete work based on client requests.
- **Background processes:** These start with the database and handle maintenance tasks such as writing data blocks to disk, maintaining online redo logs, cleaning up aborted processes, and managing the automatic workload repository.
- **Slave processes:** Similar to background processes, they perform additional work on behalf of background or server processes.

## Server Processes

Server processes execute instructions from client sessions. They receive SQL statements sent by applications and execute them within the database.

## Background Processes

An Oracle instance consists of two parts: the **SGA (System Global Area)** and a group of **background processes**. These processes work behind the scenes to ensure smooth database operation.

Background processes fall into two categories: processes with specific tasks and those that handle various other responsibilities.

1. **PMON (Process Monitor):** Monitors processes.
2. **LREG (Listener Registration Process):** Registers database services with the listener.
3. **SMON (System Monitor):** Handles system recovery.
4. **RECO (Recoverer Process):** Manages distributed database recovery.
5. **CKPT (Checkpoint Process):** Tracks checkpoints.
6. **DBWn (Database Writer Process):** Writes modified data blocks to disk.
7. **LGWR (Log Writer Process):** Writes redo log records.
8. **ARCn (Archiver Process):** Archives redo log files.
9. **DIAG (Diagnostics Process):** Handles diagnostics information.
10. **FBDA (Flashback Data Archiver):** Manages flashback archives.
11. **DBRM (Database Resource Manager):** Manages database resources.
12. **GEN0 (Generic Task Executor):** Handles general-purpose tasks.
13. **Other Common Task-Specific Processes.**

## Slave Processes

1. **I/O Slave Process:** Simulates asynchronous I/O on systems or devices that do not support it.
2. **Pnnn (Parallel Query Execution Server):** Executes parallel queries.

---

# Locks and Latches

## What is a Lock?

A lock is used to manage concurrent access to shared resources.

## Lock Issues

### Lost Updates

A lost update is a classic database problem that occurs in multi-user computing environments. It happens when:
1. A transaction in Session1 retrieves a row and displays it to User1.
2. Another transaction in Session2 retrieves the same row and displays it to User2.
3. User1 modifies the row and commits the update.
4. User2 modifies the row and commits their update, overwriting User1’s changes.

The modifications made in Step 3 are lost due to this process.

### Pessimistic Locking

Pessimistic locking is applied before a user modifies the data. This method is only suitable in **stateful** or **connected** environments, where the application maintains a continuous connection with the database during the transaction.

### Optimistic Locking

Optimistic locking delays the locking action until just before an update is executed. Users modify displayed information without locking it beforehand. This method works in all environments but increases the chance of **update failures**. If the row has already changed, the user must start over.

### Blocking

Blocking occurs when a session holds a lock on a resource while another session requests the same resource. The requesting session is blocked until the locking session releases the resource.

Blocking typically happens with these DML operations:
**INSERT, UPDATE, DELETE, MERGE, SELECT... FOR UPDATE**.

## Types of Locks

### DML Locks

DML locks control access to rows during data manipulation.

#### TX Locks (Transaction Locks)

A **TX lock** is acquired when a transaction modifies data. It is associated with the row being modified and prevents other transactions from modifying the same row.

#### TM Locks (DML Enqueue)

A **TM lock** ensures that a table’s structure is not altered while its data is modified. Unlike TX locks (one per transaction), a TM lock is acquired for each modified table.

### DDL Locks

DDL operations automatically lock objects to protect their definitions.

There are three types of DDL locks:
- **Exclusive DDL Lock:** Prevents modifications to an object while it is in use.
- **Shared DDL Lock:** Allows data modification but prevents structural changes.
- **Breakable Parse Lock:** Registers dependencies between objects, invalidating dependent objects when the base object changes.

Most DDL operations use exclusive locks.

### Latches

A **latch** is a lightweight serialization mechanism that coordinates multi-user access to shared structures, such as buffer caches or shared pools.

Latches are held briefly and cleaned up by the **PMON** process if needed.

### Mutexes

A **mutex (mutual exclusion)** is similar to a latch but is more efficient. Mutexes require less memory and fewer instructions compared to latches.

### Manual Locking and User-Defined Locks

#### Manual Locking

Oracle allows manual locking with **SELECT ... FOR UPDATE** to lock rows explicitly.

Alternatively, tables can be locked manually using **LOCK TABLE** statements.

#### Custom Locks

The **DBMS_LOCK** package lets users create custom locks for application-specific needs.

---

# Concurrency and Multi-Version Control

## What is Concurrency Control?

Concurrency control refers to the set of mechanisms in a database that allow multiple users to access and modify data simultaneously. **Locks** are a core feature that Oracle uses to manage concurrent access to shared resources and prevent interference between database transactions.

Oracle employs several types of locks, summarized below:
- **TX (Transaction) Locks:** These are acquired when a transaction modifies data.
- **TM (DML Queue) Locks and DDL Locks:** TM locks protect objects from structural changes during modifications, while DDL locks safeguard object definitions.
- **Latches and Mutexes:** These are internal Oracle locks that regulate access to shared data structures.

Oracle does not just rely on efficient locking mechanisms—it also implements a **multi-version control architecture**, enabling controlled yet highly concurrent data access. Multi-version control allows Oracle to materialize multiple versions of data simultaneously, ensuring **consistent reads**.

By default, Oracle's multi-version read consistency is **statement-level**, but it can be adjusted to **transaction-level** if needed.

## Transaction Isolation Levels

The ANSI/ISO SQL standard defines **four transaction isolation levels**, each yielding different results for the same transaction. That means two identical transactions with the same inputs may produce entirely different outcomes based on isolation levels.

These isolation levels are defined by three "phenomena" that they may allow or disallow:
- **Dirty Read:** The ability to read uncommitted data, also known as dirty data. Dirty reads affect data integrity, can break foreign key constraints, and ignore uniqueness constraints.
- **Nonrepeatable Read:** If you read a row at time T1 and then re-read it at time T2, the row may have been modified (updated or deleted) or disappeared, leading to different results.
- **Phantom Read:** If you execute a query at time T1 and then execute the same query at time T2, new rows may have been added to the database that affect your results. The difference from nonrepeatable reads is that in phantom reads, the already read data has not changed, but T2 has more data satisfying your query criteria than T1.

SQL isolation levels are defined by whether they allow the above phenomena.

| Isolation Level | Dirty Read | Nonrepeatable Read | Phantom Read |
|---|---|---|---|
| READ UNCOMMITTED | YES | YES | YES |
| READ COMMITTED | NO | YES | YES |
| REPEATABLE READ | NO | NO | YES |
| SERIALIZABLE | NO | NO | NO |

Oracle **explicitly supports** the **READ COMMITTED** and **SERIALIZABLE** isolation levels.

Oracle **does not** use dirty reads—it completely **prevents** them.

## Read Consistency

Oracle utilizes **undo records** to enable **non-blocking queries** while maintaining read consistency. When executing a query, Oracle retrieves data blocks from the buffer cache and ensures that the block versions are sufficiently **"old"** to maintain the correct visibility for the query.

## Write Consistency

Old versions of a block **cannot be modified**. Any row update must alter the **current version** of the block.

Oracle performs **two types of reads** during modification:
1. **Consistent Read:** Identifies rows to modify.
2. **Current Read:** Acquires the latest data block for actual modification.

If an `UPDATE` statement targets rows with `Y=5`, but during execution, one of those rows has changed to `Y=10`, Oracle will **internally roll back** the update and retry it.

- Under **READ COMMITTED**, Oracle silently retries the transaction without user intervention.
- Under **SERIALIZABLE**, Oracle raises an **ORA-08177: can't serialize access for this transaction** error instead of retrying.

In **READ COMMITTED** mode, if an update conflict occurs, Oracle **resets the transaction's starting point**, acquiring row locks through `SELECT FOR UPDATE`, and only **after all locks are acquired**, executes the update.

---

# Redo and Undo

Redo (redo information) is information recorded in Oracle online (or archived) redo log files, which can be used to "replay" (or redo) transactions when a database fails. Undo (rollback information) is information recorded by Oracle in undo segments, primarily used to cancel or roll back transactions.

## What is Redo

Redo log files are crucial to Oracle databases; they are the transaction logs of the database. Oracle maintains two types of redo log files: online redo log files and archived redo log files. Both types of redo log files are used for recovery, and their main purpose is to be used when a database instance or media failure occurs.

Archived redo log files are essentially copies of "old" online redo log files that have been filled. When the database fills an online redo log file, the ARCn process creates a copy of it in another location. Of course, it can also keep multiple copies locally or on a remote server.

Every Oracle database has at least two online redo log groups, and each group has at least one member (redo log file). These online redo log groups are used in a circular fashion. Oracle first writes to the log files in group 1, and when it reaches the end of the files in group 1, it switches to log file group 2 and starts writing to the files in this group. When log file group 2 is full, Oracle will switch back to log file group 1 again.

Redo logs are probably the most important recovery structure in the database, but without other components (such as undo segments, distributed transaction recovery, etc.), redo logs alone cannot do anything.

## What is Undo

Conceptually, undo is the opposite of redo. When data is modified, the database generates undo information, so that if necessary, these changes can be canceled or rolled back in the future.

Undo information is not a copy of the block before modification, but rather a set of operations that can revert the block to its previous state.

It is highly likely that blocks modified by one transaction are also being modified by other transactions at the same time. Therefore, you cannot simply revert a block to its state before the transaction began, as this would undo the work of other transactions!

## How Redo and Undo Work Together

Although undo information is stored in undo tablespaces and undo segments, it is also protected by redo. In other words, the database treats undo as it treats table data or index data; modifications to undo generate redo, which is written to the log buffer and then to the log files. Similar to non-undo data in the database, undo data is written to undo segments and also placed in the buffer cache.

### INSERT-UPDATE-DELETE-COMMIT Example Scenario

#### 1. INSERT

After an INSERT occurs, the block buffer cache contains modified undo blocks, index blocks, and table data blocks, all of which are "protected" by corresponding entries in the redo log buffer.

Before flushing the modified data blocks to disk, the redo information in the redo log buffer is written to disk. This way, if a crash occurs, all modifications can be replayed using this redo information to restore the SGA to its current state, and then the database also has corresponding undo information to roll back uncommitted transactions.

#### 2. UPDATE

UPDATE operations are largely similar to INSERTs, but UPDATEs generate more UNDO; this is because UPDATEs need to save an image of the data before modification.

---

# Indexes

## Overview

Oracle provides several different types of indexes:
- B\*Tree index: This is the most commonly used index in Oracle and most other databases. A B\*Tree is structured like a binary tree, allowing for fast access to a single row of data by its key value, or locating multiple rows within a range of key values; accessing data through this index usually only requires a few I/Os. It's important to note that the B in B\*Tree does not stand for binary, but for balanced. A B\*Tree index is not a binary tree. In addition to regular B\*Tree indexes, the following types are also considered B\*Tree indexes.
    - Index-organized table (IOT): This is a table, but its storage is also a B\*Tree structure. Data in an IOT is stored and sorted by the primary key.
    - B\*Tree cluster index: This is an approximate variant of a traditional B\*Tree index. A B\*Tree cluster index is an index built on a cluster key. In a traditional B\*Tree index, the key points to the row level; however, in a B\*Tree cluster, a cluster key points to a block that contains data related to that cluster key.
    - Descending index: In a descending index, data is arranged in "largest to smallest" order (descending), rather than "smallest to largest" (ascending).
    - Reverse key index: This is also a B\*Tree index, but the bytes within the key are "reversed." If increasing data is continuously inserted into an index, reverse key indexes will result in a more even distribution of this data. Oracle processes an index scan by reversing the bytes of the data before storing it in the index, which causes data that might have been adjacent in the original index to be placed in non-adjacent locations.

The optimizer knows what the index's column list is, and the optimizer decides whether to use your index based on the information you provide.

## B\*Tree Index

B\*Tree indexes are the most common type of index structure in databases, and their implementation is very similar to a binary search tree. Their goal is to minimize the time Oracle spends searching for data.

The blocks at the lowest level of the tree are called leaf nodes or leaf blocks, which contain individual index keys and a rowid (pointing to the indexed row). Internal blocks above the leaf nodes are called branch blocks, and data searches pass through these blocks to ultimately reach the leaf nodes.

The structure of the index leaf node level is actually a doubly linked list. If we need to search for data within a certain range (also called an index range scan), once the starting leaf node (the first value in the range) is found, the subsequent work becomes much easier. At this point, there is no need to scan the index structure from the beginning; one only needs to scan forward or backward through the leaf nodes.

One characteristic of B\*Trees is that all leaf blocks should be on the same level of the tree. This level is also called the height of the index, and all traversals from the root block of the index to a leaf block will visit the same number of blocks. The index is height-balanced. Most B\*Tree indexes have a height of 2 or 3, even with millions of records. This means that finding the first leaf block through the index only takes 2 or 3 I/Os.

---
# Database Tables

## Table Types

In Oracle, there are mainly the following 9 table types:
- Heap-organized table: This is a regular standard database table. Data is managed in it in a heap fashion. When data is added, the first free space in the segment that can accommodate the data is found and used. When data is deleted from the table, subsequent INSERTs and UPDATEs are allowed to reuse this space. This is where the name "heap" for this table type comes from. "Heap" refers to a set of spaces used in a somewhat random manner.
- Index-organized table (IOT): These tables are stored in an index structure. This enforces a physical order on the rows themselves. Unlike heap tables, where data can be placed anywhere as long as there is space, in an index-organized table (IOT), data must be stored in the IOT in primary key order.
- Index clustered table: A cluster refers to a group of one or more tables whose data is physically stored on the same database block. All rows with the same cluster key value are physically stored adjacent to each other. This structure achieves two goals. First, multiple tables can be physically stored together. Generally, you might think that a database block only contains data from one table, but for clustered tables, data from multiple tables might be stored in the same block. Second, all data containing the same cluster key value is physically stored together. Therefore, data is clustered together by the cluster key value, and the cluster key is built using a B*Tree index. The advantage of index clustered tables is that they can reduce disk I/O and improve query performance when multiple tables connected by a cluster key are frequently accessed.
- Hash clustered table: These tables are similar to index clustered tables, but instead of using a B*Tree index to locate data by the cluster key, a hash cluster hashes the key value to a cluster, directly finding the database block where the data should be. In a hash cluster, the data is the index. Hash clustered tables are suitable for frequent data reads based on key equality comparisons.
- Sorted hash clustered table: This table type was introduced in Oracle 10g. It combines some characteristics of hash clustered tables and IOTs. The concept is as follows: if your rows are hashed by a certain key value, and a series of records related to that key are written to the table in a specific sorted order and processed in that order.
- Nested table: Nested tables are part of Oracle's object-relational extensions. They are essentially child tables in a parent/child relationship that are system-generated and maintained.
- Temporary table: These tables store "draft" data during a transaction or session. Temporary tables allocate temporary segments from the current user's temporary tablespace as needed. Each session can only see the segments allocated to itself and will not see any data created by any other session. Temporary tables can be used to store temporary data, with the advantage that they generate significantly less redo compared to regular heap tables.
- Object table: Object tables are created based on an object type. They have special properties that non-object tables do not. Object tables are a special type of heap-organized table, index-organized table, and temporary table; Oracle uses these types of tables, and even nested tables, to build object tables. Additionally, nested tables are also a type of object table structure.
- External table: The data in these tables is not stored in the database but externally, meaning they are stored as regular operating system files. In Oracle 9i and later versions, external tables can be used to query files outside the database as if they were ordinary tables within the database.

Regardless of the table type, the following basic information applies:
- A table can have a maximum of 1000 columns. If a row contains more than 254 columns, Oracle internally stores it as multiple separate row pieces that point to each other, and these pieces must be reassembled into a complete row image when the row is used.
- The number of rows in a table is theoretically infinite, but due to other limitations, it cannot actually reach "infinite".
- A table can have as many indexes as there are permutations of columns.
- Even within a single database, there can be an infinite number of tables.

## Terminology

### Segment

A segment in Oracle is an object that occupies storage space on disk. There are many types of segments; the most common types are listed below:
- Cluster: This segment type can store multiple tables. There are two types of clusters: B*Tree clusters and hash clusters. Clusters are typically used to store related data from multiple tables, pre-joined and stored on the same database block; they can also be used to store related information for a single table. The term "cluster" refers to the ability of this segment to physically group related information together.
- Table: A table segment stores the data of a database table. This is probably the most commonly used segment type, often used in conjunction with index segments.
- Table partition or subpartition: This segment type is used for partitioning and is very similar to a table segment. A table partition or subpartition stores a portion of the data from a table. A partitioned table consists of one or more table partition segments, and a composite partitioned table consists of one or more table subpartition segments.
- Index: This segment type can store index structures.
- Index partition: Similar to table partitions, this segment type contains a portion of an index. A partitioned index consists of one or more index partition segments.
- LOB partition, LOB subpartition, LOB index, and LOB segment: LOB index and LOB segment types can store large object (LOB) structures. When partitioning a table that contains LOBs, the LOB segments are also partitioned, and LOB partition segments are used for this purpose.
- Nested table: This is the segment type specified for nested tables, which are a special type of child table in a parent/child relationship.
- Rollback segment and "Type2 undo" segment: Undone data is stored here. Rollback segments are segments manually created by DBAs. Type2 undo segments are automatically created and managed by Oracle.

### Segment Space Management

Starting from Oracle 9i, there are two methods for managing segment space:
- Manual Segment Space Management (MSSM): You set FREELISTS, FREELIST GROUPS, PCTUSED, and other parameters to control how space within segments is allocated, used, and reused.
- Automatic Segment Space Management (ASSM): You only need to control one parameter related to space usage, PCTFREE.

### High-Water Mark

If you imagine a table as a flat structure, or a series of blocks arranged from left to right, the High-Water Mark (HWM) is the rightmost block that has ever contained data.

# Data Types

## Overview

Oracle provides 22 different SQL data types for use, briefly introduced as follows:
- CHAR: This is a fixed-length string that will be padded with spaces to reach its maximum length. A non-null CHAR(10) contains 10 **bytes** of information. A CHAR field can store up to 2000 bytes of information.
- NCHAR: This is a fixed-length string containing UNICODE formatted data. A non-null NCHAR(10) always contains 10 **characters** of information. An NCHAR field can store up to 2000 bytes of information.
- VARCHAR2: This is a synonym for VARCHAR. This is a variable-length string. Unlike the CHAR type, it will not pad the field or variable with spaces to its maximum length. VARCHAR2(10) can contain 0 to 10 bytes of information, and it can store up to 4000 bytes of information. Starting from Oracle 12c, it can store up to 32767 bytes of information.
- NVARCHAR2: This is a variable-length string containing UNICODE formatted data. NVARCHAR2(10) can contain 0 to 10 **characters** of information, and NVARCHAR2 can store up to 4000 bytes of information. Starting from Oracle 12c, it can store up to 32767 bytes of information.
- RAW: This is a variable-length binary data type, meaning that data stored using this data type will not undergo character set conversion. This type can store up to 2000 bytes of information. Starting from Oracle 12c, it can store up to 32767 bytes of information.
- NUMBER: This data type can store numbers with a precision of up to 38 digits, ranging from 1.0\*10^(-130) to 1.0\*10(126) (exclusive). Numbers of this type are stored in a variable-length format, with a length of 0 to 22 bytes (NULL values have a length of 0).
- BINARY_FLOAT: A 32-bit single-precision floating-point number, supporting at least 6 digits of precision, occupying 5 bytes of storage on disk.
- BINARY_DOUBLE: A 64-bit double-precision floating-point number, supporting at least 15 digits of precision, occupying 9 bytes of storage on disk.
- LONG: Stores up to 2GB of character data. The LONG type is provided only for backward compatibility, so it is strongly recommended not to use the LONG type in new applications; use the CLOB type instead.
- LONG RAW: The LONG RAW type can store up to 2GB of raw binary information. For the same reasons as LONG, it is recommended that all newly developed applications use the BLOB type.
- DATE: This is a 7-byte fixed-width date/time data type, containing 7 attributes in total: century, year within century, month, day within month, hour, minute, and second.
- TIMESTAMP: This is a 7-byte or 11-byte fixed-width date/time data type (higher precision uses 11 bytes). TIMESTAMP can include fractional seconds; TIMESTAMP with fractional seconds can retain up to 9 decimal places.
- TIMESTAMP WITH TIME ZONE: Similar to the previous type, this is a 13-byte fixed-width TIMESTAMP, but it also provides time zone support. Because time zone information is stored with the TIMESTAMP, the time zone information at insertion will be preserved along with the time.
- TIMESTAMP WITH LOCAL TIME ZONE: Similar to TIMESTAMP, this is a 7-byte or 11-byte fixed-width date/time data type (higher precision uses 11 bytes); however, this type is time zone sensitive. If data of this type is inserted or modified, the database will normalize the date/time part of the data, converting it to the database's time zone, by referencing the TIME ZONE provided in the data and the database's own time zone.
- INTERVAL YEAR TO MONTH: This is a 5-byte fixed-width data type used to store a period of time. This type stores the period as years and months.
- INTERVAL DAY TO SECOND: This is an 11-byte fixed-width data type used to store a period of time. This type stores the period as days/hours/minutes/seconds, and can also have up to 9 decimal places for fractional seconds.
- BLOB: In Oracle 10g and later versions, it can store up to (4GB - 1)*(database block size) bytes of data. BLOB contains "binary" data that does not require character set conversion.
- CLOB: In Oracle 10g and later versions, it can store up to (4GB - 1)*(database block size) bytes of data. CLOB is affected by character set conversion. This data type is well-suited for storing large blocks of plain text information.
- NCLOB: In Oracle 10g and later versions, it can store up to (4GB - 1)*(database block size) bytes of data. NCLOB stores information encoded in the database's national character set, and like CLOB, this type is also affected by character set conversion.
- BFILE: This data type can store an Oracle directory object and a filename in a database column, allowing us to read this file. This effectively allows you to access operating system files on the database server in a read-only manner, as if they were stored in a database table.
- ROWID: ROWID is actually the address of a row in a database table; it is 10 bytes long. The information encoded in a ROWID is sufficient not only to locate each row on disk but also to identify the object to which the ROWID points.
- UROWID: UROWID is a universal ROWID used for tables without a fixed ROWID (such as IOTs and tables accessed via heterogeneous database gateways). UROWID typically represents the value of the primary key, so the size of a UROWID varies depending on the object it points to.

Types like INT, INTEGER, SMALLINT, FLOAT, REAL, etc., are actually implemented based on one of the fundamental types listed above; in other words, they are synonyms for Oracle's intrinsic types.

## Character and Binary String Types

Character data types in Oracle include CHAR, VARCHAR2, and their corresponding N-prefixed variants (NCHAR and NVARCHAR2). CHAR and NCHAR types can store 2000 bytes of text, while VARCHAR2 and NVARCHAR2 can hold 4000 bytes.

Data in CHAR, VARCHAR2, NCHAR, and NVARCHAR2 will be converted between different character sets by the database as needed.

# Parallel Execution

Parallel execution is a feature available only in Oracle Enterprise Edition (not in Standard Edition).

Parallel execution refers to the ability to physically divide a large serial task (including all DML and general DDL) into multiple smaller parts, which can be processed simultaneously.
- Parallel query: This refers to the ability to use multiple operating system processes or threads to execute a query. Oracle will identify operations that can be executed in parallel (such as full table scans or large-scale sorting) and create a query plan to achieve parallel execution.
- Parallel DML (PDML): This is essentially very similar to parallel query, but PDML primarily uses parallel processing to perform modifications (INSERT, UPDATE, DELETE, and MERGE).
- Parallel DDL: Parallel DDL refers to Oracle's ability to execute large-scale DDL operations in parallel. For example, index rebuilding, creating a new index, data loading via CREATE TABLE AS SELECT, and reorganization of large tables can all use parallel processing.
- Parallel loading: External tables and SQL\*Loader can load data in parallel.
- Procedural parallelization: This refers to the ability to run developed code in parallel.

There are also two other operations that can be implemented in parallel:
- Parallel recovery: Oracle can parallelize database recovery operations.
- Parallel propagation: Parallel execution also has a more typical data replication scenario, where Oracle Advanced Replication options can perform asynchronous parallel replication, and parallel mode can significantly improve the throughput of data replication operations.

## When to Use Parallel Execution

>The parallel query (PARALLEL QUERY) option is inherently not scalable.

Parallel execution is inherently a non-scalable solution, designed to allow a single user or a specific SQL statement to occupy all database resources. If a feature allows one person to use all available resources, running two people using this feature will encounter significant contention issues. As the number of concurrent users on the system increases, the...

# Partitioning

Partitioning allows a table or index to be physically divided into multiple smaller, more manageable pieces. Although a partitioned table or index may consist of dozens of physical partitions, from the perspective of the application accessing the database, it accesses a single logical table or index. Each partition is an independent object that can be processed individually or as part of a larger object.

## Partitioning Overview

Partitioning uses a "divide and conquer" approach, suitable for managing very large tables and indexes. Partitioning introduces the concept of a partition key. Data is distributed to corresponding partitions based on its partition key value. The method of partitioning can be based on a range of key values, a list of key values, or a hash function value of the partition key. Here are some benefits of partitioning:
- Improved data availability: This applies to any type of system, regardless of whether it is primarily an OLTP or data warehouse system.
- Breaking down large segments into smaller segments, thereby reducing management burden: Performing management operations on a 100GB table is much more burdensome than performing the same operation 10 times on individual 10GB table partitions. Additionally, by using partitioning, we can delete data without leaving fragmented space, thus eliminating the need to reorganize the table!
- Improved performance for certain queries: This is mainly for large data warehousing environments. By using partitioning, we can skip data in certain partitions, thereby narrowing the range of data that needs to be accessed and processed. However, this is not applicable in transactional systems, as such systems typically access only small amounts of data.
- Distributing data modifications across multiple partitions, thereby reducing contention on high-load OLTP systems: If an application experiences severe contention for a certain segment, we can divide it into multiple segments, which can proportionally reduce contention.

### Increased Availability

Increased availability stems from the independence of each partition. The availability (or unavailability) of one partition in a table (index) does not affect the availability of the table (index) itself. If your table (index) is partitioned, the query optimizer will be aware of this and eliminate unnecessary partitions from the execution plan. For example, if one partition in a large object is unavailable, but your query does not need that partition, Oracle can still successfully process the query.

### Reduced Management Burden

The partitioning mechanism reduces the management burden because performing the same operation on smaller objects is easier, faster, and consumes fewer resources compared to performing it on a large object.

### Enhanced Statement Performance

The third benefit of partitioning is its ability to enhance the performance of some SQL statements (SELECT, INSERT, UPDATE, DELETE, MERGE). These SQL statements fall into two categories: those that modify information and those that read information.

1. Parallel DML
Statements that modify data in the database can be executed in parallel (Parallel DML, PDML). When executed in PDML mode, Oracle uses multiple threads or processes to perform INSERT, UPDATE, DELETE, or MERGE, rather than executing them serially in a single process. On a multi-CPU host with sufficient I/O bandwidth, this large-scale operation...

