
## Chapter 1: Selectivity and Cardinality

Performance tuning is one area where most Junior DBAs face those tough-to-break problems. The kind of problems which require deep understanding of the concepts before you can even point what is going wrong in the system. We will be going through the basics knowledge in the Performance Tuning area in series of posts.

### Selectivity

**Definition**: It represents the fraction of rows filtered by an operation, so you can say it is a measure of uniqueness.

#### Key Characteristics:
- **Range**: Its value is between 0 and 1
- **Calculation**: selectivity = (rows returned after filter) / (total rows before filter)

#### Examples:

**Example 1 - Good Selectivity:**
- Query returned 100 rows initially
- After applying filter (WHERE condition), final result is 10 rows
- Selectivity = 10/100 = 0.1 (or 10%)
- This is **GOOD selectivity** because the filter significantly reduced the result set

**Example 2 - Bad Selectivity:**
- Query returned 200 rows initially  
- After applying filter, final result is still 200 rows
- Selectivity = 200/200 = 1.0 (or 100%)
- This is **BAD selectivity** because the filter didn't reduce the result set at all

#### Selectivity Classifications:

**GOOD Selectivity:**
- A column is highly selective if a SQL returns a small number of duplicate rows
- Means the filter is effective at narrowing down results
- Results in better performance

**BAD Selectivity:**
- A column is least selective if a SQL returns all or large number of rows
- Means the filter is not effective
- Results in poor performance

#### Important Notes:
- When you run `SELECT * FROM EMP` without any filters, selectivity will be automatically 1 as all rows will be returned
- **Adding a composite Index** is the best way to make BAD selectivity become GOOD selectivity
- Using more than one column makes the Index more unique which improves Index selectivity

### Cardinality

**Definition**: The number of rows returned by an operation is the cardinality.

#### Relationship Formula:
```
cardinality = selectivity × number of input rows
```

#### Practical Example:
- Query initially retrieved 200 records from database
- After applying filters, final number of rows is 50
- **Selectivity** = 50/200 = 0.25 (25%)
- **Cardinality** = 50 (or 200 × 0.25 = 50)

#### Impact on Performance:
Sometimes the Oracle optimizer is not able to predict the number of rows that a given operator will return due to reasons like:
- Missing table statistics
- Outdated statistics
- Complex predicates

This can prevent Oracle from estimating the cost of a query plan correctly, which can lead to:
- Selection of suboptimal execution plans
- Cardinality estimation errors
- Slow running queries

### Detailed Examples

#### Example 1: Query Without Filter
```sql
SELECT MAX(EMP_NUMBER) FROM EMP;
```
**Scenario**: Table EMP has 10 records total

**Analysis**:
- **Selectivity** = number of rows accessed / total number of rows = 10/10 = 1.0
- **Interpretation**: 100% of the rows were accessed
- **Cardinality** = number of rows accessed = 10

#### Example 2: Query With Filter  
```sql
SELECT MAX(EMP_NUMBER) FROM EMP WHERE LAST_NAME = 'SMITH';
```
**Scenario**: Only 4 employees have LAST_NAME as 'SMITH' out of 10 total records

**Analysis**:
- **Selectivity** = number of rows accessed / total number of rows = 4/10 = 0.4
- **Interpretation**: 40% of the rows were accessed
- **Cardinality** = number of rows accessed = 4

This demonstrates how adding a selective filter condition improves selectivity from 1.0 to 0.4, making the query more efficient.

---

## Chapter 2: Parsing

From performance tuning perspectives, it is very important to understand the concept of parsing. Parsing is the primary phase in SQL execution, followed by other stages: Execute and Fetch.

### Parsing Basics

Whenever a SQL statement is executed, the Oracle Engine performs the following actions:

1. **Validate the Syntax** - Check if the SQL statement is syntactically correct
2. **Validate the Objects** - Verify that all objects referenced in the statement exist
3. **Check Privileges** - Ensure the user has necessary privileges to execute the statement
4. **Search Shared Pool** - Verify if the statement is already available in the shared pool by:
   - Oracle engine calculates the hash value for the SQL statement
   - Looks in the shared pool for matching hash
5. **Allocate Memory** - If statement is not present, allocate shared memory and create a cursor in shared pool
6. **Generate Execution Plan** - Create the optimal execution plan for the statement

### Types of Parses

#### Hard Parse

**Definition**: A hard parse occurs when the statement is not available in shared memory or this is a brand new statement that the user is trying to execute.

**When Hard Parse Occurs**:
- Statement has never been executed before
- Statement was aged out of shared pool due to memory pressure
- Statement text doesn't exactly match existing statements (even case sensitivity matters)

**Process**: All parsing steps (1-6 above) need to be completed

**Impact**: 
- Requires extra system resources
- CPU-intensive operation
- Also known as **'Library Cache Miss'**

#### Soft Parse

**Definition**: A soft parse occurs when the statement was executed earlier, was already parsed, and is available in memory.

**Process**: Oracle only needs to perform steps 1-3 (syntax validation, object validation, privilege check) since the remaining tasks were already completed earlier.

**Benefits**:
- Much faster than hard parse
- Minimal resource consumption
- Also known as **'Library Cache Hit'**
- Follows the principle: "work hard once and reap benefits multiple times"

### Why Hard Parses Should Be Avoided

There are two key reasons why hard parses should be kept to the bare minimum required:

#### 1. CPU Intensive Operations
- **Generation of an execution plan is a very CPU-intensive operation**
- Each hard parse consumes significant CPU resources
- High hard parse rates can lead to CPU bottlenecks

#### 2. Memory Serialization Issues
- **Memory in the shared pool is limited**
- **Memory operations are serialized** - they must happen one at a time
- Memory operations use **shared pool latches** and **library cache latches**
- When many hard parses happen simultaneously:
  - Other processes must wait in queue to get the shared pool latch
  - This creates contention and reduces overall system performance
  - Impacts both shared pool latch and library cache latch availability

### Performance Implications

#### Hard Parse Impact:
- High CPU consumption
- Memory contention
- Increased response times
- Reduced throughput
- Latch waits

#### Soft Parse Benefits:
- Low CPU consumption  
- Reduced memory operations
- Faster response times
- Higher throughput
- Better scalability

### Best Practices

1. **Use Bind Variables** - Promotes statement reuse and soft parsing
2. **Consistent SQL Text** - Ensure identical statements have identical text (case, spacing, etc.)
3. **Adequate Shared Pool Size** - Prevent aging out of frequently used statements
4. **Monitor Parse Ratios** - Track hard vs soft parse ratios
5. **Application Design** - Design applications to reuse SQL statements

### Monitoring Parsing

Key metrics to monitor:
- Hard parse rate
- Soft parse rate  
- Parse time CPU vs total CPU
- Library cache hit ratio
- Shared pool latch contention

---

## Chapter 3: Parent and Child Cursors

### What is a Cursor?

A "cursor" is a memory area in the library cache that is allocated to the SQL statement which users execute. This memory area stores key information about the SQL statement like SQL text, SQL execution plan, statistics etc.

### Why Two Kinds of Cursors?

This is by Oracle database design that you have two kinds of cursors: Parent and Child. For each SQL statement that you execute, Oracle engine will generate two cursors: parent and child cursor. Two cursors are generated because for the same SQL statement, there could be other differences like there can be different bind values or two different schema or different literals values, etc. The parent Cursor will hold the SQL statement and the child cursor will hold the information related to the differences. This essentially makes child cursor as deciding factor as to SQL statement will go for hard or soft parse.

### Parent Cursor

- It stores the SQL text of the cursor. When two statements are identical word-by-word, they will share the same parent Cursor.
- Every parent cursor would execute with at least one child cursor created for it.
- Parent cursors are represented in the view **V$SQLAREA**. VERSION_COUNT column in the v$sqlarea can tell us how many child cursors does this parent cursor have.

### Child Cursor

- Each parent has at least one child cursor and can have more than 1 child cursors also
- While parent cursor stores the SQL Text, the child cursor stores other important information related to SQL statement like:
  - Environment details
  - Statistics details
  - Bind Variables details
  - Execution Plan details
  - Bind Variables details
- Child Cursor takes less memory space as SQL Text is not stored in child cursor
- Every child cursor must belong to a parent
- Child cursor decides whether a query will undergo a hard parse or a soft parse. You may find situation that SQL query is same for two statements so Parent cursors are same but the child cursor is not shareable to SQL goes for hard parse (re-compile).
- Child cursors are represented in the view **V$SQL**
- **V$SQL_SHARED_CURSOR** is very useful view as it provides the reasons why the optimizer decided mark the cursor as un-shared. So anytime you see that SQL statement was same and still hard parse happened, look at this view.

### V$SQL_SHARED_CURSOR View

This view explains why a particular child cursor is not shared with existing child cursors which caused more than one child cursor to be created for same Parent cursor. Each column in this view identifies a specific reason why the cursor cannot be shared. The columns describe the various reasons with "Y" or "N" for the value. You should focus on the column which has value as 'Y'. A specific child may have failed sharing for several reasons – ie: a different reason for trying to use different existing child cursors.

### CURSOR_SHARING Database Parameter

Since we are discussing parent and child cursors, it is very important that we discuss a bit about the cursor_sharing database parameter. CURSOR_SHARING determines what kind of SQL statements can share the same cursors.

cursor_sharing database parameters can have three different values:

#### EXACT
Only allows statements with identical text to share the same cursor.

#### FORCE
Forces statements that may differ in some literals, but are otherwise identical, to share a cursor, unless the literals affect the meaning of the statement.

#### SIMILAR
Causes statements that may differ in some literals, but are otherwise identical, to share a cursor, unless the literals affect either the meaning of the statement or the degree to which the plan is optimized.

### Examples of Different SQL Statements

The default cursor_sharing criteria is EXACT which means that each different SQL statement a new parent cursor is created. Example, below are two different SQL statements:

```sql
select * from EMP WHERE EMP_ID=1;
select * from EMP where EMP_ID=1;
```

These are two different SQLs although both will produce same result. "where" is written in capital letter in first statement while in the second statement it is written in small letters.

Even below two statements are two different SQLs:

```sql
select * from EMP where EMP_ID=1;
select * from EMP where EMP_ID=2;
```

These are different SQLs as literal values (1 and 2) are different. Executing above will create two Parent cursors if cursor_sharing parameter is EXACT.

Whereas, if you put cursor_sharing criteria is FORCE or SIMILAR, executing above two SQLs will generate single Parent cursor. When we do this, Oracle strips out all the literals from the query and replaces them with bind variables in the optimization phase. Please keep in mind that making cursor_sharing is not always an advantage. It can prove bad for SQL performance also as we will discuss in further posts.

---

## Chapter 4: Bind Variables

### Introduction to Bind Variables

Bind variables are often known as one of the key feature for better SQL query performance. Bind variables as per Oracle documentation is a placeholder in a SQL statement that must be replaced with a valid value or value address for the statement to execute successfully. By using bind variables, you can write a SQL statement that accepts inputs or parameters at run time.

You can think of SQL query as a kind of "function" in any programming language and bind variables as "values" that you pass to the function.

### Example

```sql
-- Without bind variable (using literal)
Select * from EMP where EMP_ID=1;

-- With bind variable
Select * from EMP where EMP_ID=:a;
```

First statement uses a literal value (1) to run the query while the second SQL statement uses bind variable (:a) to run the SQL statement. The value of (:a) will be provide to Oracle at run time.

### Key Benefits of Bind Variables

Having bind variable defined in the SQL query instead of literal values (which can be different every time) will make sure that Oracle will create only one Parent Cursor for the SQL statement. Oracle look for exact text match for the SQL statement to see if it is already present in the shared pool and having a bind variable instead of literal value will save a costly hard parse every time same SQL is executed.

Bind variables are specially important in OLTP kind of environments as using bind variables enables soft parsing, which means that less processing time is spent on choosing an optimized execution plan.

### Creating Bind Variables in SQL*Plus

You create bind variables in SQL*Plus with the VARIABLE command. Example:

```sql
VARIABLE mybindVariable VARCHAR2(10)
```

### Advantages of Using Bind Variables

1. **Better Shared Pool Utilization**: Oracle Shared Pool has to hold only one statement rather than a potentially very high number.

2. **No Hard Parsing so Better Performance**: No hard parsing required for SQL statements that only differ in the values.

3. **Reduced "library cache" latch contention**: Bind variables helps in avoiding performance problems due to library cache latch contention which happens every time a hard parse is required.

### Disadvantages of Using Bind Variables

Now coming to disadvantages of using bind variables. Note that in many cases bind variables will prove excellent for improving the performance of the database but at time it may produce negative results. Bind variables can reduce the information to calculate optimal access path for (Cost Based Optimizer) CBO.

---
# Chapter 5 Oracle Database Tuning: Comprehensive Notes on Trace and TKPROF

## Table of Contents
1. [Overview](#overview)
2. [Prerequisites and Key Parameters](#prerequisites-and-key-parameters)
3. [Part 1: Trace Generation Methods](#part-1-trace-generation-methods)
4. [Part 2: Generating TKPROF from Trace Files](#part-2-generating-tkprof-from-trace-files)
5. [Part 3: Analyzing TKPROF Files](#part-3-analyzing-tkprof-files)
6. [Oracle EBS Specific Tracing](#oracle-ebs-specific-tracing)
7. [Best Practices and Additional Tips](#best-practices-and-additional-tips)

---

## Overview

### What is Oracle Trace?
**Trace** is an Oracle feature that generates a raw text file containing internal SQL activity when a program is executed. It provides detailed statistics for performance analysis and troubleshooting.

**Key Information Captured:**
- Parse, execute, and fetch counts
- CPU and elapsed times
- Physical reads and logical reads
- Number of rows processed
- Library cache misses
- Username under each parse occurred
- Commit and rollback operations
- Wait events (when enabled)

### What is TKPROF?
**TKPROF** reformats raw trace data into a readable format for performance analysis. It doesn't control trace contents but makes analysis easier.

**Two Essential Goals:**
1. Identify SQL/Operations taking the highest time
2. Identify SQL/Operations consuming the highest resources

---

## Prerequisites and Key Parameters

### Essential Database Parameters

```sql
-- Enable timing statistics
ALTER SYSTEM SET TIMED_STATISTICS = TRUE;

-- Allow unlimited trace file size
ALTER SYSTEM SET MAX_DUMP_FILE_SIZE = UNLIMITED;

-- Comprehensive statistics collection
ALTER SYSTEM SET STATISTICS_LEVEL = ALL;
```

**Parameter Details:**
- **TIMED_STATISTICS**: Must be TRUE for meaningful timing data
- **MAX_DUMP_FILE_SIZE**: Prevents truncated trace files
- **STATISTICS_LEVEL**: ALL provides most comprehensive information

### Trace Levels Overview

| Level | Description | Use Case |
|-------|-------------|----------|
| 1 | Standard SQL Trace | Basic tracing |
| 4 | Includes Bind Variables | When bind values needed |
| 8 | **With Waits (Recommended)** | **Default choice - shows wait events** |
| 12 | Binds + Waits | Comprehensive but overhead-heavy |
| 16 | STAT lines for each execution | Equivalent to plan_stat=ALL_EXECUTIONS |
| 32 | Never write STAT lines | Equivalent to plan_stat=NEVER |
| 64 | STAT lines every minute | Adaptive statistics (11.2.0.2 only) |

**Best Practice:** Use Level 8 (with waits) by default. Only use Level 12 when bind variable values are specifically needed.

---

## Part 1: Trace Generation Methods

### 1. Session-Level Tracing

#### Basic SQL_TRACE
```sql
-- Enable trace for current session
ALTER SESSION SET SQL_TRACE = TRUE;

-- Disable trace
ALTER SESSION SET SQL_TRACE = FALSE;

-- 11g+ Event Syntax
ALTER SESSION SET EVENTS 'sql_trace bind=true';
ALTER SESSION SET EVENTS 'sql_trace bind=true, wait=true';
```

#### 10046 Event Tracing (Recommended)
```sql
-- Level 1 (Basic)
ALTER SESSION SET EVENTS '10046 trace name context forever';

-- Level 8 (With Waits - Recommended)
ALTER SESSION SET EVENTS '10046 trace name context forever, level 8';

-- Level 12 (Binds and Waits)
ALTER SESSION SET EVENTS '10046 trace name context forever, level 12';

-- Disable tracing
ALTER SESSION SET EVENTS '10046 trace name context off';
```

### 2. SQL_ID Specific Tracing (11g+)
```sql
-- Single SQL_ID
ALTER SYSTEM SET EVENTS 'sql_trace [sql: sql_id=4k1jlmn567cr7] bind=true, wait=true';

-- Multiple SQL_IDs
ALTER SYSTEM SET EVENTS 'sql_trace [sql: sql_id=5t6ygtsa3d356|6fa43fgg0rrtp] bind=true, wait=true';
```

### 3. Database-Level Tracing
```sql
-- System-wide (Use with caution!)
ALTER SYSTEM SET SQL_TRACE = TRUE;
ALTER SYSTEM SET EVENTS '10046 trace name context forever, level 8';

-- Disable system-wide
ALTER SYSTEM SET SQL_TRACE = FALSE;
ALTER SYSTEM SET EVENTS '10046 trace name context off';
```

**Warning:** System-wide tracing creates significant overhead. Use only for short periods in controlled environments.

### 4. DBMS_MONITOR Package

#### Session Tracing
```sql
-- Enable for specific session
EXEC DBMS_MONITOR.SESSION_TRACE_ENABLE(
  session_id => &SESSION_ID, 
  serial_num => &SERIAL_NUM, 
  waits => TRUE, 
  binds => FALSE
);

-- Disable for specific session
EXEC DBMS_MONITOR.SESSION_TRACE_DISABLE(
  session_id => &SESSION_ID, 
  serial_num => &SERIAL_NUM
);
```

#### Client ID Tracing
```sql
-- Enable for client identifier
EXEC DBMS_MONITOR.CLIENT_ID_TRACE_ENABLE(
  client_id => 'client_identifier',
  waits => TRUE,
  binds => FALSE
);

-- Disable
EXEC DBMS_MONITOR.CLIENT_ID_TRACE_DISABLE(
  client_id => 'client_identifier'
);
```

### 5. DBMS_SESSION Package
```sql
-- Find session details
SELECT sid, serial#, username, program 
FROM v$session 
WHERE username = 'TARGET_USER';

-- Enable tracing
EXEC DBMS_SESSION.SET_SQL_TRACE_IN_SESSION(&SID, &SERIAL#, TRUE);

-- Disable tracing
EXEC DBMS_SESSION.SET_SQL_TRACE_IN_SESSION(&SID, &SERIAL#, FALSE);
```

### 6. ORADEBUG Utility

**Steps:**
1. **Find Process Information:**
```sql
SELECT p.PID, p.SPID, s.SID 
FROM v$process p, v$session s 
WHERE s.paddr = p.addr 
AND s.sid = &SESSION_ID;
```

2. **Apply Trace:**
```sql
CONNECT / AS SYSDBA
ORADEBUG SETOSPID &SPID
ORADEBUG UNLIMIT
ORADEBUG EVENT 10046 TRACE NAME CONTEXT FOREVER, LEVEL 8
```

3. **Disable Trace:**
```sql
ORADEBUG EVENT 10046 TRACE NAME CONTEXT OFF
```

### 7. User-Level Tracing with Triggers
```sql
CREATE OR REPLACE TRIGGER SYS.set_trace 
AFTER LOGON ON DATABASE 
WHEN (USER like '&USERNAME') 
DECLARE 
  lcommand varchar(200); 
BEGIN 
  EXECUTE IMMEDIATE 'ALTER SESSION SET TRACEFILE_IDENTIFIER=''From_Trigger'''; 
  EXECUTE IMMEDIATE 'ALTER SESSION SET STATISTICS_LEVEL=ALL'; 
  EXECUTE IMMEDIATE 'ALTER SESSION SET MAX_DUMP_FILE_SIZE=UNLIMITED'; 
  EXECUTE IMMEDIATE 'ALTER SESSION SET EVENTS ''10046 trace name context forever, level 8'''; 
END set_trace;
/
```

### 8. AUTOTRACE in SQL*Plus
```sql
-- Show execution plan only
SET AUTOTRACE TRACEONLY EXPLAIN

-- Show statistics only
SET AUTOTRACE TRACEONLY STATISTICS

-- Show both
SET AUTOTRACE TRACEONLY

-- Show results + plan + statistics
SET AUTOTRACE ON
```

---

## Trace File Locations and Naming

### File Locations

**Pre-11g:**
- User sessions: `user_dump_dest`
- Background processes: `background_dump_dest`

**11g and Later:**
```sql
-- Find trace directory
SELECT value FROM V$DIAG_INFO WHERE NAME = 'Diag Trace';
```
Location: `<diagnostic_dest>/diag/rdbms/<dbname>/<instname>/trace`

### File Naming Convention
Format: `<instance>_ora_<ospid>_<identifier>.trc`

**Components:**
- `<instance>`: Database instance name
- `<ospid>`: OS Process ID from v$process.spid
- `<identifier>`: Trace file identifier

---

## Part 2: Generating TKPROF from Trace Files

### Basic TKPROF Syntax
```bash
tkprof <input_trace_file> <output_file> [options]
```

### Essential TKPROF Command
```bash
tkprof ora123.trc ora123.out sort=fchela,exeela,prsela sys=no
```

### Key TKPROF Parameters

| Parameter | Description | Example |
|-----------|-------------|---------|
| **sort** | Sort statements by resource usage | `sort=fchela,exeela,prsela` |
| **sys** | Include/exclude SYS recursive SQL | `sys=no` (recommended) |
| **print** | Limit number of statements | `print=10` |
| **explain** | Generate execution plans | `explain=user/pass@db` |
| **table** | Specify plan table | `table=sys.plan_table` |
| **record** | Create replay script | `record=replay.sql` |
| **width** | Control output line width | `width=132` |

### Sort Options (Most Useful)

| Sort Code | Description |
|-----------|-------------|
| **fchela** | Fetch elapsed time |
| **exeela** | Execute elapsed time |
| **prsela** | Parse elapsed time |
| **fchdsk** | Fetch disk reads |
| **exedsk** | Execute disk reads |
| **prsdsk** | Parse disk reads |
| **fchqry** | Fetch buffer gets |
| **exeqry** | Execute buffer gets |
| **prsqry** | Parse buffer gets |

### Common TKPROF Examples

```bash
# Basic sorted output
tkprof trace.trc output.out sort=fchela,exeela,prsela sys=no

# Top 10 resource-intensive statements
tkprof trace.trc output.out sort=exeela,fchela print=10 sys=no

# With execution plans
tkprof trace.trc output.out explain=system/password@orcl sys=no

# Complete analysis with custom table
tkprof trace.trc output.out sort=fchela,exeela explain=system/pass table=my_plan_table sys=no
```

### TKPROF Output Components

**What TKPROF Contains:**
1. **SQL Text** - Executed statements
2. **Timing Information** - Parse/Execute/Fetch times
3. **Resource Usage** - CPU, disk reads, buffer gets
4. **Wait Information** - Wait events (if trace level 8+)
5. **Execution Plans** - Runtime execution paths
6. **Row Counts** - Actual rows processed

---

## Part 3: Analyzing TKPROF Files

### TKPROF File Structure

#### 1. Header Section
- TKPROF version and generation time
- Source trace file name
- Sort options used
- Column definitions

#### 2. Body Section (Main Analysis Area)
Each SQL statement contains:
- SQL text
- Parse/Execute/Fetch statistics
- Library cache information
- Row source execution plan
- Wait events (if available)

#### 3. Summary Section
- Overall statistics
- Recursive vs non-recursive SQL
- Library cache hit ratios
- Total elapsed time

### Understanding Body Section Metrics

#### Performance Metrics Table

| Metric | Unit | Description |
|--------|------|-------------|
| **count** | Number | Times parsed, executed, or fetched |
| **cpu** | Seconds | CPU time consumed |
| **elapsed** | Seconds | Total elapsed time |
| **disk** | Blocks | Physical reads from disk |
| **query** | Blocks | Logical reads (consistent mode) |
| **current** | Blocks | Logical reads (current mode) |
| **rows** | Number | Rows processed |

#### Call Types Explained

| Call Type | Description | When It Occurs |
|-----------|-------------|----------------|
| **Parse** | SQL parsing and optimization | First time SQL is seen |
| **Execute** | Statement execution | Every execution |
| **Fetch** | Row retrieval | SELECT statements only |

### Key Analysis Techniques

#### 1. Identify Performance Issues
```
Look for:
- High elapsed times
- High CPU consumption
- Excessive physical reads (disk)
- High parse counts
- Poor fetch patterns
```

#### 2. Parse Analysis
```
Hard Parse Indicators:
- Misses in library cache during parse = 1
- High parse times
- Parse count close to execute count

Soft Parse (Good):
- Misses in library cache during parse = 0
- Low parse times
- Parse count << execute count
```

#### 3. I/O Analysis
```
Physical vs Logical Reads:
- High disk reads = I/O bound
- High query/current = Memory intensive
- Buffer Cache Hit Ratio = 1 - (disk/(query+current))

Target: >95% hit ratio in most cases
```

#### 4. Row Source Operations
```sql
-- Example execution plan output
Rows (1st) Rows (avg) Rows (max)  Row Source Operation
---------- ---------- ----------  -------------------
         1          1          1  UPDATE CUSTOMERS (cr=4 pr=0 pw=0 time=0 us)
         1          1          1   INDEX UNIQUE SCAN CUSTOMERS_PK (cr=3 pr=0 pw=0 time=0 us)
```

**Row Source Metrics:**
- **cr**: Consistent reads (query mode)
- **pr**: Physical reads
- **pw**: Physical writes
- **time**: Time in microseconds

### Summary Section Analysis

#### Library Cache Hit Ratio Calculation
```
Physical reads = sum(disk)
Logical reads = sum(query + current)
Hit Ratio = 1 - (Physical Reads / Logical Reads)

Target: > 95% for most applications
```

#### Key Summary Metrics
- **Parse efficiency**: Total parses vs total SQL statements
- **Fetch efficiency**: Fetches vs rows returned
- **Overall elapsed time**: Total time for all operations

### Wait Events Analysis (Level 8+ Traces)

| Wait Event | Meaning | Action |
|------------|---------|--------|
| **db file sequential read** | Single block reads | Check index usage, disk I/O |
| **db file scattered read** | Multi-block reads | Full table scans |
| **latch: shared pool** | Library cache contention | Reduce hard parsing |
| **enq: TX - row lock contention** | Row locking issues | Application logic review |

---

## Oracle EBS Specific Tracing

### 1. Forms-Level Tracing

**Steps:**
1. Set profile: `Utilities:Diagnostics = Yes`
2. Navigate to target form
3. Help → Diagnostics → Trace → Trace with Waits
4. Select "Unlimited Trace File Size"
5. Execute required actions
6. Help → Diagnostics → Trace → No Trace

### 2. Self-Service/OAF Tracing

**Steps:**
1. Set profile: `FND: Diagnostics = Yes`
2. Navigate to HTML application
3. Click Diagnostics icon → Set Trace Level → Trace with Waits
4. Execute required actions
5. Diagnostics icon → Set Trace Level → Disable Trace

### 3. Concurrent Program Tracing

#### Request Level (R12+ only)
1. Set profile: `Concurrent: Allow Debugging = Yes`
2. Submit Request → Debug Options
3. Enable "SQL Trace" → "SQL Trace with Waits"
4. Submit request

#### Program Level
1. Navigate to Concurrent → Program → Define
2. Query target program
3. Check "Enable Trace" checkbox
4. Submit requests will be traced

### 4. Profile Option Method
Set `Initialization SQL Statement – Custom` profile:
```sql
BEGIN 
  FND_CTL.FND_SESS_CTL(','
    ,'TRUE'
    ,'TRUE'
    ,''
    ,'ALTER SESSION SET TRACEFILE_IDENTIFIER="custom_trace" 
      STATISTICS_LEVEL=ALL 
      MAX_DUMP_FILE_SIZE=unlimited 
      EVENTS="10046 TRACE NAME CONTEXT FOREVER, LEVEL 8"'
  );
END;
```

---

## Best Practices and Additional Tips

### 1. Trace Management
```bash
# Monitor trace directory size
du -sh $ORACLE_BASE/diag/rdbms/*/trace

# Clean up old traces
find $TRACE_DIR -name "*.trc" -mtime +7 -delete

# Compress large traces
gzip large_trace.trc
```

### 2. TRCSESS Utility (10g+)
Consolidate multiple trace files:
```bash
trcsess output=consolidated.trc session=123 *.trc
trcsess output=client_trace.trc clientid=BATCH_USER *.trc
```

### 3. Performance Analysis Workflow

1. **Enable appropriate trace level**
2. **Execute problematic operation**  
3. **Generate TKPROF with proper sorting**
4. **Analyze in order:**
   - Summary section (overall impact)
   - Top resource-consuming SQLs
   - Parse efficiency
   - Wait events
   - Execution plans

### 4. Common Issues and Solutions

| Issue | Symptom | Solution |
|-------|---------|----------|
| **Over-parsing** | Parse count ≈ Execute count | Use bind variables |
| **Full table scans** | High "disk" reads | Add/modify indexes |
| **Inefficient joins** | High row counts in plans | Review join order |
| **Lock contention** | High TX wait events | Review transaction logic |

### 5. Additional Analysis Tools

#### SQL Performance Scripts
```sql
-- Find expensive SQL by elapsed time
SELECT sql_id, elapsed_time, executions, 
       elapsed_time/executions avg_elapsed
FROM v$sql 
WHERE executions > 0
ORDER BY elapsed_time DESC;

-- Check current sessions with tracing
SELECT s.sid, s.serial#, s.username, s.program,
       p.tracefile
FROM v$session s, v$process p
WHERE s.paddr = p.addr
AND s.sql_trace = 'enabled';
```

#### Monitoring Queries
```sql
-- Find trace files by session
SELECT s.sid, s.serial#, s.username,
       p.spid, p.tracefile
FROM v$session s, v$process p  
WHERE s.paddr = p.addr
AND s.sid = &SESSION_ID;

-- Check enabled traces
SELECT trace_type, primary_id, qualifier_id1, 
       waits, binds 
FROM dba_enabled_traces;
```

### 6. Advanced Techniques

#### Custom TKPROF Analysis
```bash
# Create custom analysis script
tkprof input.trc output.out sort=fchela,exeela,prsela sys=no | \
grep -E "(elapsed|CPU|disk)" > performance_summary.txt

# Extract only top SQLs
tkprof input.trc output.out print=5 sort=exeela sys=no
```

#### Automated Analysis
```sql
-- PL/SQL block for automated trace analysis
DECLARE
  v_tracefile VARCHAR2(500);
  v_sql VARCHAR2(1000);
BEGIN
  SELECT p.tracefile INTO v_tracefile
  FROM v$session s, v$process p
  WHERE s.paddr = p.addr
  AND s.sid = SYS_CONTEXT('userenv','sid');
  
  DBMS_OUTPUT.PUT_LINE('Trace file: ' || v_tracefile);
  
  -- Enable tracing
  EXECUTE IMMEDIATE 'ALTER SESSION SET EVENTS ''10046 trace name context forever, level 8''';
  
  -- Your SQL operations here
  
  -- Disable tracing  
  EXECUTE IMMEDIATE 'ALTER SESSION SET EVENTS ''10046 trace name context off''';
END;
/
```

### 7. Security and Considerations

- **Sensitive Data**: Level 12 traces contain bind variables (potential sensitive data)
- **File System Impact**: Large traces can fill disk space
- **Performance Overhead**: Tracing adds 5-10% overhead
- **Production Use**: Always coordinate with stakeholders
- **Cleanup**: Establish trace file retention policies

### 8. Troubleshooting Common Issues

| Problem | Possible Cause | Solution |
|---------|----------------|----------|
| **No trace file generated** | Wrong parameters/permissions | Check diagnostic_dest, permissions |
| **Empty TKPROF** | Trace disabled prematurely | Verify trace duration |
| **Missing wait events** | Trace level < 8 | Use level 8 or 12 |
| **Huge trace files** | Long-running operations | Use print parameter, monitor size |

---

## Summary

Oracle trace and TKPROF are powerful tools for database performance analysis. Key takeaways:

1. **Use Level 8 tracing** (with waits) for most scenarios
2. **Sort TKPROF output** by elapsed time for efficiency
3. **Focus on high-impact SQLs** first
4. **Analyze parse efficiency** to identify optimization opportunities
5. **Monitor wait events** to understand bottlenecks
6. **Clean up trace files** regularly to prevent space issues

The combination of proper tracing methodology and systematic TKPROF analysis provides deep insights into database performance issues and guides effective optimization strategies.



## Chapter 6: Optimizer Mode - Advanced Deep Dive

### Introduction to Oracle Cost-Based Optimizer (CBO)

The Oracle Cost-Based Optimizer is a sophisticated component that uses mathematical models and statistical analysis to determine the most efficient execution path for SQL statements. It replaced the older Rule-Based Optimizer (RBO) and represents the evolution of Oracle's query optimization technology.

#### CBO Architecture Components

1. **Query Transformer**: Rewrites queries for better performance
2. **Estimator**: Calculates costs, cardinalities, and selectivities
3. **Plan Generator**: Creates alternative execution plans
4. **Plan Selector**: Chooses the plan with the lowest cost

#### Cost Calculation Formula
```
Cost = (CPU_Cost / CPU_SPEED) + (IO_Cost / IO_SPEED)
```

Where:
- CPU_Cost = Number of CPU operations required
- IO_Cost = Number of I/O operations required
- CPU_SPEED and IO_SPEED are system-dependent factors

### Detailed Analysis of Optimizer Modes

#### 1. FIRST_ROWS Mode - Deep Analysis

**Technical Implementation:**
- Uses a modified cost formula that heavily weights early row retrieval
- Prefers nested loop joins over hash joins
- Favors index range scans over full table scans
- Applies different costing for sorting operations

**Mathematical Model:**
```
First_Rows_Cost = α × Initial_Response_Cost + β × Total_Cost
```
Where α > β, giving higher weight to initial response time.

**Use Cases and Scenarios:**
- Interactive applications with pagination
- OLTP systems with user interfaces
- Applications using ROWNUM or FETCH FIRST clauses
- Real-time dashboards requiring immediate feedback

**Performance Characteristics:**
```sql
-- Example showing FIRST_ROWS behavior
SELECT /*+ FIRST_ROWS */ employee_id, employee_name, salary
FROM employees 
WHERE department_id = 10
ORDER BY salary DESC;

-- Typical execution plan characteristics:
-- 1. INDEX RANGE SCAN (instead of FULL TABLE SCAN)
-- 2. NESTED LOOPS (instead of HASH JOIN)
-- 3. SORT ORDER BY STOPKEY (optimized sorting)
```

**Potential Drawbacks:**
- Higher overall resource consumption
- Increased elapsed time for complete result sets
- May create more expensive plans for batch operations

#### 2. FIRST_ROWS_N Mode - Precision Optimization

**Algorithm Details:**
The optimizer uses a cost model that specifically optimizes for retrieving exactly N rows:

```sql
-- Different N values produce different optimization strategies
SELECT /*+ FIRST_ROWS(1) */ * FROM large_table WHERE condition;   -- Single row optimization
SELECT /*+ FIRST_ROWS(10) */ * FROM large_table WHERE condition;  -- Small batch optimization
SELECT /*+ FIRST_ROWS(100) */ * FROM large_table WHERE condition; -- Medium batch optimization
SELECT /*+ FIRST_ROWS(1000) */ * FROM large_table WHERE condition;-- Large batch optimization
```

**Optimization Strategies by N Value:**

| N Value | Primary Strategy | Secondary Strategy | Join Method |
|---------|-----------------|-------------------|-------------|
| 1 | Unique index access | Range scan with ROWNUM | Nested loop |
| 10 | Index range scan | Partial sort | Nested loop |
| 100 | Index/partial scan | Hash join (small) | Hash/Nested |
| 1000+ | Mixed approach | Full optimization | All methods |

**Advanced Example:**
```sql
-- Analyzing the impact of different N values
EXPLAIN PLAN FOR
SELECT /*+ FIRST_ROWS(1) */ e.employee_id, e.name, d.department_name
FROM employees e, departments d
WHERE e.department_id = d.department_id
  AND e.salary > 50000;

-- vs.

EXPLAIN PLAN FOR
SELECT /*+ FIRST_ROWS(1000) */ e.employee_id, e.name, d.department_name
FROM employees e, departments d
WHERE e.department_id = d.department_id
  AND e.salary > 50000;
```

#### 3. ALL_ROWS Mode - Throughput Optimization

**Core Philosophy:**
Minimizes total system resources (CPU + I/O) required to complete the entire query.

**Optimization Techniques:**
- **Parallel Processing**: Automatically considers parallel execution
- **Hash Joins**: Prefers hash joins for large data sets
- **Full Table Scans**: Uses FTS when it's more efficient than index access
- **Sort-Merge Joins**: Utilizes when beneficial for large sorted datasets

**Resource Management:**
```sql
-- ALL_ROWS mode resource allocation example
SELECT /*+ ALL_ROWS PARALLEL(e,4) PARALLEL(d,4) */ 
       e.employee_id, e.name, d.department_name, SUM(s.salary_amount)
FROM employees e, departments d, salary_history s
WHERE e.department_id = d.department_id
  AND e.employee_id = s.employee_id
  AND s.salary_date >= '2023-01-01'
GROUP BY e.employee_id, e.name, d.department_name;
```

**Cost Model Differences:**
```
ALL_ROWS_Cost = Total_CPU_Cost + Total_IO_Cost + Parallel_Overhead
FIRST_ROWS_Cost = Weighted_Initial_Cost + Reduced_Total_Cost
```

### Advanced Optimizer Mode Configuration

#### System-Level Optimization

**Instance-Wide Settings:**
```sql
-- Set optimizer mode with additional parameters
ALTER SYSTEM SET optimizer_mode = 'ALL_ROWS' SCOPE=BOTH;
ALTER SYSTEM SET optimizer_index_cost_adj = 10 SCOPE=BOTH;
ALTER SYSTEM SET optimizer_index_caching = 90 SCOPE=BOTH;

-- Monitor the changes
SELECT name, value, isdefault, ismodified 
FROM v$parameter 
WHERE name LIKE 'optimizer%';
```

**Advanced Parameter Tuning:**
```sql
-- Fine-tune optimizer behavior
ALTER SYSTEM SET optimizer_dynamic_sampling = 4;  -- Increased sampling for better statistics
ALTER SYSTEM SET optimizer_adaptive_features = TRUE;  -- Enable adaptive optimizations
ALTER SYSTEM SET optimizer_adaptive_plans = TRUE;     -- Enable adaptive plans
```

#### Session-Level Customization

**Complex Session Configuration:**
```sql
-- Comprehensive session setup for different workload types
-- OLTP Workload Configuration
ALTER SESSION SET optimizer_mode = FIRST_ROWS;
ALTER SESSION SET optimizer_index_cost_adj = 10;
ALTER SESSION SET cursor_sharing = EXACT;
ALTER SESSION SET optimizer_use_invisible_indexes = FALSE;

-- Data Warehouse Configuration  
ALTER SESSION SET optimizer_mode = ALL_ROWS;
ALTER SESSION SET parallel_degree_policy = ADAPTIVE;
ALTER SESSION SET optimizer_adaptive_features = TRUE;
ALTER SESSION SET "_optimizer_use_feedback" = TRUE;
```

#### Application-Specific Optimizer Configuration

**Using Database Services:**
```sql
-- Create service for OLTP workload
BEGIN
  DBMS_SERVICE.CREATE_SERVICE(
    service_name => 'OLTP_SERVICE',
    network_name => 'OLTP_SERVICE'
  );
  
  -- Set optimizer preferences for the service
  DBMS_SERVICE.MODIFY_SERVICE(
    service_name => 'OLTP_SERVICE',
    parameter_name => 'OPTIMIZER_MODE',
    parameter_value => 'FIRST_ROWS'
  );
END;
/
```

**Logon Triggers for User-Based Optimization:**
```sql
CREATE OR REPLACE TRIGGER optimize_by_user
AFTER LOGON ON DATABASE
DECLARE
  v_username VARCHAR2(30) := USER;
BEGIN
  CASE 
    WHEN v_username LIKE 'APP_%' THEN
      EXECUTE IMMEDIATE 'ALTER SESSION SET optimizer_mode = FIRST_ROWS';
      EXECUTE IMMEDIATE 'ALTER SESSION SET optimizer_index_cost_adj = 10';
      
    WHEN v_username LIKE 'ETL_%' THEN
      EXECUTE IMMEDIATE 'ALTER SESSION SET optimizer_mode = ALL_ROWS';
      EXECUTE IMMEDIATE 'ALTER SESSION SET parallel_degree_policy = ADAPTIVE';
      
    WHEN v_username LIKE 'RPT_%' THEN
      EXECUTE IMMEDIATE 'ALTER SESSION SET optimizer_mode = ALL_ROWS';
      EXECUTE IMMEDIATE 'ALTER SESSION SET optimizer_adaptive_features = TRUE';
      
    ELSE
      NULL; -- Use default settings
  END CASE;
  
  -- Log the optimization choice
  INSERT INTO optimizer_audit_log (username, login_time, optimizer_mode)
  VALUES (v_username, SYSDATE, SYS_CONTEXT('USERENV', 'OPTIMIZER_MODE'));
  COMMIT;
END;
/
```

### Monitoring and Troubleshooting Optimizer Behavior

#### Real-Time Monitoring Queries

**Current Session Optimizer Settings:**
```sql
-- Comprehensive optimizer configuration view
SELECT 
    s.sid,
    s.serial#,
    s.username,
    s.program,
    p.name AS parameter_name,
    p.value AS parameter_value,
    p.isdefault,
    p.ismodified
FROM v$session s,
     v$parameter p
WHERE s.sid = SYS_CONTEXT('USERENV', 'SID')
  AND p.name IN ('optimizer_mode', 'optimizer_index_cost_adj', 
                 'optimizer_index_caching', 'optimizer_dynamic_sampling')
ORDER BY p.name;
```

**Execution Plan Analysis:**
```sql
-- Analyze execution plans by optimizer mode
SELECT 
    sql_id,
    plan_hash_value,
    optimizer_mode,
    optimizer_cost,
    executions,
    elapsed_time/executions/1000000 as avg_elapsed_sec,
    buffer_gets/executions as avg_buffer_gets,
    disk_reads/executions as avg_disk_reads
FROM v$sql
WHERE optimizer_mode IS NOT NULL
  AND executions > 0
ORDER BY avg_elapsed_sec DESC;
```

#### Performance Impact Assessment

**Before/After Analysis Framework:**
```sql
-- Create performance baseline
CREATE TABLE optimizer_baseline AS
SELECT 
    sql_id,
    plan_hash_value,
    optimizer_mode,
    executions,
    elapsed_time,
    cpu_time,
    buffer_gets,
    disk_reads,
    SYSDATE as baseline_date
FROM v$sql
WHERE last_active_time > SYSDATE - 1;

-- Compare performance after optimizer mode changes
SELECT 
    b.sql_id,
    b.optimizer_mode as old_mode,
    s.optimizer_mode as new_mode,
    ROUND((s.elapsed_time/s.executions)/(b.elapsed_time/b.executions), 2) as elapsed_ratio,
    ROUND((s.buffer_gets/s.executions)/(b.buffer_gets/b.executions), 2) as buffer_ratio,
    ROUND((s.disk_reads/s.executions)/(b.disk_reads/b.executions), 2) as disk_ratio
FROM optimizer_baseline b,
     v$sql s
WHERE b.sql_id = s.sql_id
  AND b.optimizer_mode != s.optimizer_mode
  AND s.executions > 10
ORDER BY elapsed_ratio DESC;
```

### Best Practices and Advanced Techniques

#### Workload Classification Framework

**Automatic Workload Classification:**
```sql
-- Create workload classification function
CREATE OR REPLACE FUNCTION classify_workload(
    p_sql_text CLOB,
    p_rows_processed NUMBER,
    p_executions NUMBER
) RETURN VARCHAR2 IS
    v_classification VARCHAR2(20);
BEGIN
    -- Rule-based classification
    IF UPPER(p_sql_text) LIKE '%SELECT%' AND 
       UPPER(p_sql_text) LIKE '%ROWNUM%' THEN
        v_classification := 'OLTP_PAGINATION';
    ELSIF p_rows_processed / p_executions > 10000 THEN
        v_classification := 'BATCH_PROCESSING';
    ELSIF UPPER(p_sql_text) LIKE '%GROUP BY%' OR 
          UPPER(p_sql_text) LIKE '%ORDER BY%' THEN
        v_classification := 'ANALYTICAL';
    ELSE
        v_classification := 'OLTP_STANDARD';
    END IF;
    
    RETURN v_classification;
END;
/

-- Apply classification to recommend optimizer mode
SELECT 
    sql_id,
    classify_workload(sql_fulltext, rows_processed, executions) as workload_type,
    CASE classify_workload(sql_fulltext, rows_processed, executions)
        WHEN 'OLTP_PAGINATION' THEN 'FIRST_ROWS_10'
        WHEN 'BATCH_PROCESSING' THEN 'ALL_ROWS'
        WHEN 'ANALYTICAL' THEN 'ALL_ROWS'
        ELSE 'FIRST_ROWS'
    END as recommended_optimizer_mode,
    optimizer_mode as current_mode
FROM v$sql
WHERE executions > 5
  AND last_active_time > SYSDATE - 1;
```

---

## Chapter 7: Trace File Analyzer (TRCA) - Complete Implementation Guide

### TRCA Architecture and Components

#### Internal Architecture

**TRCA System Components:**
1. **TRCANLZR Schema**: Core analysis engine
2. **Trace Parser**: Interprets raw trace files
3. **Statistics Collector**: Gathers database metadata
4. **Report Generator**: Produces HTML/Text outputs
5. **Performance Repository**: Stores historical analysis data

**Database Objects Created:**
```sql
-- Key TRCA objects (created during installation)
SELECT object_name, object_type, status 
FROM dba_objects 
WHERE owner = 'TRCANLZR'
ORDER BY object_type, object_name;

-- Sample output includes:
-- TRCA$_TRACE_FILES (Table)
-- TRCA$_SQL_STATEMENTS (Table)  
-- TRCA$_EXECUTION_PLANS (Table)
-- TRCA_PKG (Package)
-- Various views and indexes
```

### Advanced Installation and Configuration

#### Pre-Installation Assessment

**System Requirements Check:**
```sql
-- Check database version compatibility
SELECT banner FROM v$version WHERE banner LIKE 'Oracle%';

-- Verify available tablespace space
SELECT 
    tablespace_name,
    ROUND(bytes/1024/1024, 2) as size_mb,
    ROUND(maxbytes/1024/1024, 2) as max_size_mb,
    ROUND((bytes - NVL(free_bytes, 0))/1024/1024, 2) as used_mb,
    ROUND(NVL(free_bytes, 0)/1024/1024, 2) as free_mb
FROM (
    SELECT 
        df.tablespace_name,
        SUM(df.bytes) as bytes,
        SUM(df.maxbytes) as maxbytes,
        SUM(fs.bytes) as free_bytes
    FROM dba_data_files df,
         (SELECT tablespace_name, SUM(bytes) as bytes 
          FROM dba_free_space GROUP BY tablespace_name) fs
    WHERE df.tablespace_name = fs.tablespace_name(+)
    GROUP BY df.tablespace_name
)
WHERE tablespace_name NOT IN ('SYSTEM', 'SYSAUX');
```

**Installation Parameter Optimization:**
```sql
-- Create dedicated tablespace for TRCA
CREATE TABLESPACE TRCA_DATA
DATAFILE '/u01/oradata/ORCL/trca_data01.dbf' SIZE 500M
AUTOEXTEND ON NEXT 100M MAXSIZE 2G
EXTENT MANAGEMENT LOCAL
SEGMENT SPACE MANAGEMENT AUTO;

CREATE TABLESPACE TRCA_TEMP
TEMPFILE '/u01/oradata/ORCL/trca_temp01.dbf' SIZE 100M
AUTOEXTEND ON NEXT 50M MAXSIZE 1G;
```

#### Detailed Installation Process

**Enhanced Installation Script:**
```sql
-- Enhanced TRCA installation with custom parameters
-- File: enhanced_tacreate.sql

-- Set installation parameters
DEFINE trca_password = 'SecurePassword123'
DEFINE trca_tablespace = 'TRCA_DATA'
DEFINE trca_temp_tablespace = 'TRCA_TEMP'
DEFINE trca_user = 'APPS'
DEFINE staging_objects = 'T'

-- Execute installation with parameters
@tacreate.sql

-- Post-installation verification
SELECT username, account_status, default_tablespace, temporary_tablespace
FROM dba_users 
WHERE username = 'TRCANLZR';

-- Verify granted roles and privileges
SELECT grantee, granted_role, admin_option
FROM dba_role_privs
WHERE grantee = 'TRCANLZR'
UNION ALL
SELECT grantee, privilege, admin_option
FROM dba_sys_privs
WHERE grantee = 'TRCANLZR'
ORDER BY 2;
```

### Comprehensive Trace Analysis Workflow

#### Advanced Trace Generation

**Multi-Level Trace Generation:**
```sql
-- Session-level trace with enhanced information
ALTER SESSION SET events '10046 trace name context forever, level 12';
ALTER SESSION SET timed_statistics = TRUE;
ALTER SESSION SET statistics_level = ALL;
ALTER SESSION SET max_dump_file_size = UNLIMITED;

-- Execute target SQL
SELECT /*+ FULL(e) FULL(d) USE_HASH(e,d) */ 
       e.employee_id, e.first_name, e.last_name, d.department_name
FROM hr.employees e, hr.departments d
WHERE e.department_id = d.department_id
  AND e.hire_date >= DATE '2020-01-01';

-- Disable tracing
ALTER SESSION SET events '10046 trace name context off';
```

**System-Level Trace for Multiple Sessions:**
```sql
-- Enable tracing for specific module
BEGIN
    DBMS_MONITOR.SERV_MOD_ACT_TRACE_ENABLE(
        service_name => 'ORCL',
        module_name  => 'MYAPP',
        action_name  => 'REPORT_GENERATION',
        waits        => TRUE,
        binds        => TRUE,
        plan_stat    => 'ALL_EXECUTIONS'
    );
END;
/

-- Disable module tracing
BEGIN
    DBMS_MONITOR.SERV_MOD_ACT_TRACE_DISABLE(
        service_name => 'ORCL',
        module_name  => 'MYAPP',
        action_name  => 'REPORT_GENERATION'
    );
END;
/
```

#### Advanced TRCA Execution

**Batch Processing Multiple Traces:**
```sql
-- Create batch processing script
-- File: batch_trca_analysis.sql

SET SERVEROUTPUT ON
DECLARE
    CURSOR c_trace_files IS
        SELECT filename
        FROM (
            SELECT value || '/' || name as filename
            FROM v$diag_info
            WHERE name LIKE '%trc'
        );
    
    v_cmd VARCHAR2(1000);
BEGIN
    FOR rec IN c_trace_files LOOP
        v_cmd := 'start trcanlzr.sql ' || rec.filename;
        DBMS_OUTPUT.PUT_LINE('Processing: ' || rec.filename);
        
        -- Execute TRCA for each trace file
        EXECUTE IMMEDIATE 'ALTER SESSION SET current_schema = TRCANLZR';
        -- Add actual TRCA execution logic here
        
    END LOOP;
END;
/
```

**Automated TRCA Report Generation:**
```bash
#!/bin/bash
# Script: auto_trca_analysis.sh

# Configuration
ORACLE_SID=ORCL
TRACE_DIR=/u01/app/oracle/diag/rdbms/orcl/ORCL/trace
REPORT_DIR=/u01/reports/trca
DATE_SUFFIX=$(date +%Y%m%d_%H%M%S)

# Find recent trace files
find $TRACE_DIR -name "*.trc" -mtime -1 > /tmp/trace_files_$DATE_SUFFIX.txt

# Process each trace file
while read trace_file; do
    if [ -f "$trace_file" ]; then
        echo "Processing $trace_file"
        
        # Connect to database and run TRCA
        sqlplus -S apps/password@$ORACLE_SID << EOF
        start /path/to/trca/run/trcanlzr.sql $trace_file
        exit;
EOF
        
        # Move generated reports
        mv *.html $REPORT_DIR/
        mv *.txt $REPORT_DIR/
    fi
done < /tmp/trace_files_$DATE_SUFFIX.txt

# Cleanup
rm /tmp/trace_files_$DATE_SUFFIX.txt
```

### TRCA Report Analysis and Interpretation

#### Report Structure Deep Dive

**HTML Report Sections:**
1. **Executive Summary**: High-level performance metrics
2. **SQL Statements Analysis**: Individual SQL performance breakdown
3. **Execution Plans**: Detailed plan analysis with costs
4. **Wait Events Analysis**: Bottleneck identification
5. **System Statistics**: Database configuration impact
6. **Recommendations**: Actionable tuning suggestions

**Key Performance Indicators in Reports:**

```sql
-- Query to extract key metrics from TRCA repository
SELECT 
    sql_id,
    parsing_schema_name,
    sql_text_snippet,
    executions,
    ROUND(elapsed_time_total/1000000, 2) as elapsed_seconds,
    ROUND(cpu_time_total/1000000, 2) as cpu_seconds,
    ROUND(elapsed_time_total/executions/1000000, 2) as avg_elapsed,
    buffer_gets_total,
    disk_reads_total,
    rows_processed_total,
    ROUND(buffer_gets_total/rows_processed_total, 2) as buffers_per_row
FROM trca$_sql_exec_summary
WHERE executions > 0
ORDER BY elapsed_time_total DESC;
```

#### Advanced Report Customization

**Custom Report Templates:**
```sql
-- Create custom TRCA report template
CREATE OR REPLACE PROCEDURE generate_custom_trca_report(
    p_trace_filename VARCHAR2,
    p_report_title VARCHAR2 DEFAULT 'Custom TRCA Analysis'
) IS
    v_html CLOB;
BEGIN
    -- Custom HTML report generation logic
    v_html := '<!DOCTYPE html><html><head><title>' || p_report_title || '</title>';
    v_html := v_html || '<style>
        body { font-family: Arial, sans-serif; }
        .header { background-color: #f0f0f0; padding: 10px; }
        .metric { margin: 5px 0; }
        .sql-text { background-color: #f9f9f9; padding: 10px; font-family: monospace; }
        table { border-collapse: collapse; width: 100%; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
    </style></head><body>';
    
    -- Add custom sections
    v_html := v_html || '<div class="header"><h1>' || p_report_title || '</h1>';
    v_html := v_html || '<p>Generated: ' || TO_CHAR(SYSDATE, 'YYYY-MM-DD HH24:MI:SS') || '</p></div>';
    
    -- Add performance summary
    FOR rec IN (
        SELECT sql_id, elapsed_time_total, executions, buffer_gets_total
        FROM trca$_sql_exec_summary
        WHERE trace_filename = p_trace_filename
        ORDER BY elapsed_time_total DESC
        FETCH FIRST 10 ROWS ONLY
    ) LOOP
        v_html := v_html || '<div class="metric">SQL ID: ' || rec.sql_id || 
                           ', Elapsed: ' || ROUND(rec.elapsed_time_total/1000000, 2) || 's</div>';
    END LOOP;
    
    v_html := v_html || '</body></html>';
    
    -- Save custom report
    -- Implementation for saving CLOB to file
    NULL;
END;
/
```

### Integration with Other Oracle Tools

#### TRCA + AWR Integration

**Correlating TRCA with AWR Data:**
```sql
-- Find AWR snapshots corresponding to trace period
WITH trace_period AS (
    SELECT 
        MIN(timestamp) as trace_start,
        MAX(timestamp) as trace_end
    FROM trca$_trace_entries
    WHERE trace_filename = 'your_trace_file.trc'
)
SELECT 
    s.snap_id,
    s.begin_interval_time,
    s.end_interval_time,
    'BEFORE_TRACE' as period_type
FROM dba_hist_snapshot s, trace_period tp
WHERE s.end_interval_time <= tp.trace_start
  AND s.begin_interval_time >= tp.trace_start - INTERVAL '2' HOUR

UNION ALL

SELECT 
    s.snap_id,
    s.begin_interval_time,
    s.end_interval_time,
    'DURING_TRACE' as period_type
FROM dba_hist_snapshot s, trace_period tp
WHERE s.begin_interval_time <= tp.trace_end
  AND s.end_interval_time >= tp.trace_start

UNION ALL

SELECT 
    s.snap_id,
    s.begin_interval_time,
    s.end_interval_time,
    'AFTER_TRACE' as period_type
FROM dba_hist_snapshot s, trace_period tp
WHERE s.begin_interval_time >= tp.trace_end
  AND s.end_interval_time <= tp.trace_end + INTERVAL '2' HOUR

ORDER BY begin_interval_time;
```

#### TRCA + SQL Monitor Integration

**Enhanced Monitoring Setup:**
```sql
-- Enable SQL monitoring for traced sessions
ALTER SESSION SET "_sqlmon_threshold" = 0;  -- Monitor all SQL
ALTER SESSION SET control_management_pack_access = 'DIAGNOSTIC+TUNING';

-- Create monitoring report for traced SQL
SELECT 
    DBMS_SQLTUNE.REPORT_SQL_MONITOR(
        sql_id => 'your_sql_id',
        session_id => your_session_id,
        type => 'HTML',
        report_level => 'ALL'
    ) as sql_monitor_report
FROM dual;
```

### Troubleshooting TRCA Issues

#### Common Installation Problems

**Tablespace Issues:**
```sql
-- Diagnose tablespace problems
SELECT 
    'TRCANLZR user tablespace usage' as check_type,
    tablespace_name,
    ROUND(bytes/1024/1024, 2) as used_mb,
    blocks,
    segments
FROM (
    SELECT 
        s.tablespace_name,
        SUM(s.bytes) as bytes,
        SUM(s.blocks) as blocks,
        COUNT(*) as segments
    FROM dba_segments s
    WHERE s.owner = 'TRCANLZR'
    GROUP BY s.tablespace_name
);

-- Check for quota issues
SELECT username, tablespace_name, bytes, max_bytes
FROM dba_ts_quotas
WHERE username = 'TRCANLZR';
```

**Permission Issues:**
```sql
-- Verify required privileges
SELECT 
    'Missing Privileges' as issue_type,
    required_priv
FROM (
    SELECT 'SELECT ANY DICTIONARY' as required_priv FROM dual
    UNION ALL SELECT 'CREATE SESSION' FROM dual
    UNION ALL SELECT 'CREATE TABLE' FROM dual
    UNION ALL SELECT 'CREATE SEQUENCE' FROM dual
    UNION ALL SELECT 'CREATE PROCEDURE' FROM dual
) required
WHERE required_priv NOT IN (
    SELECT privilege 
    FROM dba_sys_privs 
    WHERE grantee = 'TRCANLZR'
    UNION
    SELECT privilege 
    FROM dba_sys_privs 
    WHERE grantee IN (
        SELECT granted_role 
        FROM dba_role_privs 
        WHERE grantee = 'TRCANLZR'
    )
);
```

#### Runtime Analysis Problems

**Trace File Access Issues:**
```bash
# Check trace file permissions and ownership
ls -la $ORACLE_BASE/diag/rdbms/*/trace/*.trc

# Verify Oracle user can read trace files
sudo -u oracle cat /path/to/trace/file.trc | head -10

# Check for trace file corruption
grep -c "PARSING IN CURSOR" /path/to/trace/file.trc
grep -c "EXEC" /path/to/trace/file.trc
grep -c "FETCH" /path/to/trace/file.trc
```

**Performance Optimization for TRCA:**
```sql
-- Optimize TRCA performance
ALTER SESSION SET optimizer_mode = ALL_ROWS;
ALTER SESSION SET sort_area_size = 134217728;  -- 128MB
ALTER SESSION SET hash_area_size = 134217728;  -- 128MB
ALTER SESSION SET parallel_degree_policy = ADAPTIVE;

-- Monitor TRCA execution
SELECT 
    s.sid,
    s.serial#,
    s.username,
    s.program,
    s.sql_id,
    s.sql_exec_start,
    s.last_call_et,
    s.status
FROM v$session s
WHERE s.username = 'TRCANLZR'
   OR s.program LIKE '%trcanlzr%';
```

# Oracle Database Performance Tuning Notes for Beginners

## Chapter 8: Understanding Histograms

###  What You'll Learn in This Chapter
- What histograms are and why they matter
- How histograms help Oracle make better decisions
- When to use histograms and when to avoid them
- Practical examples you can follow

---

###  What is a Histogram? 

Think of a histogram like a **bar chart that shows how your data is spread out**.

**Real-world analogy:** Imagine you're a store manager tracking sales by country:
- You sold 1000 items total
- 800 went to USA, 150 to Canada, 50 to Mexico

A histogram would show this as three bars with different heights, making it easy to see that USA gets most of your sales.

**In database terms:** A histogram tells Oracle how your data is distributed across different values in a column.

---

###  Why Do We Need Histograms?

**The Problem:** Oracle is like a GPS system for your data queries. Without good information, it might choose a slow route.

**Example Scenario:**
```
Table: CUSTOMER_ORDERS
Total rows: 10,000
Column: COUNTRY
Data distribution:
- USA: 9,000 customers
- Canada: 500 customers  
- Mexico: 500 customers
```

**Without Histogram (Oracle's Assumption):**
- Oracle thinks: "3 countries, so probably 3,333 customers each"
- For query: `SELECT * FROM CUSTOMER_ORDERS WHERE COUNTRY = 'Mexico'`
- Oracle expects 3,333 rows, so it chooses a slow method

**With Histogram (Reality):**
- Oracle knows: "Mexico only has 500 customers"
- Oracle chooses a faster method for finding just 500 rows

---

### 📈 Types of Histograms (Simplified)

#### 1. **Frequency Histograms** (Most Common)
- **What it is:** Each unique value gets its own bucket
- **When Oracle uses it:** When you have few unique values
- **Think of it as:** Individual boxes for each country

**Example:**
```
Bucket 1: USA (9,000 customers)
Bucket 2: Canada (500 customers)
Bucket 3: Mexico (500 customers)
```

#### 2. **Height-Balanced Histograms**
- **What it is:** Values are grouped into equal-sized buckets
- **When Oracle uses it:** When you have many unique values
- **Think of it as:** Grouping similar values together

**Example:**
```
Bucket 1: Countries with 0-3,333 customers
Bucket 2: Countries with 3,334-6,666 customers
Bucket 3: Countries with 6,667-10,000 customers
```

---

### 🔍 How to Check if Your Table Uses Histograms

**Simple Query:**
```sql
-- Replace 'YOUR_TABLE' and 'YOUR_COLUMN' with actual names
SELECT 
    column_name,
    num_buckets,
    histogram
FROM USER_TAB_COL_STATISTICS 
WHERE table_name = 'YOUR_TABLE' 
AND column_name = 'YOUR_COLUMN';
```

**What to look for:**
- `num_buckets > 1` = Histogram exists
- `histogram = 'FREQUENCY'` or `'HEIGHT BALANCED'` = Type of histogram

---

### ⚙️ Key Settings for Beginners

#### The METHOD_OPT Parameter (Don't worry, it's simpler than it sounds!)

**What it does:** Tells Oracle when to create histograms automatically

**Default setting (recommended for beginners):**
```sql
-- Check current setting
SELECT dbms_stats.get_prefs('METHOD_OPT') FROM dual;
```

**Typical result:** `FOR ALL COLUMNS SIZE AUTO`
- This means: "Oracle, please create histograms automatically when you think they're needed"

**If you want to change it:**
```sql
-- Let Oracle decide automatically (RECOMMENDED)
EXEC dbms_stats.set_global_prefs('METHOD_OPT','FOR ALL COLUMNS SIZE AUTO');

-- Turn off histograms completely (NOT recommended)
EXEC dbms_stats.set_global_prefs('METHOD_OPT','FOR ALL COLUMNS SIZE 1');
```

---

### 🛠️ Managing Histograms (Beginner Examples)

#### Removing a Histogram (if it's causing problems)
```sql
BEGIN 
    dbms_stats.delete_column_stats(
        ownname => 'YOUR_SCHEMA',  -- Replace with your schema name
        tabname => 'YOUR_TABLE',   -- Replace with your table name
        colname => 'YOUR_COLUMN',  -- Replace with your column name
        col_stat_type => 'HISTOGRAM'
    ); 
END; 
/
```

#### Preventing Future Histograms on Specific Columns
```sql
BEGIN 
    dbms_stats.set_table_prefs(
        'YOUR_SCHEMA',  -- Your schema
        'YOUR_TABLE',   -- Your table
        'METHOD_OPT' => 'FOR ALL COLUMNS SIZE AUTO, FOR COLUMNS SIZE 1 YOUR_COLUMN'
    ); 
END; 
/
```

---

### ✅ When Should You Use Histograms? (Beginner Guidelines)

#### **👍 CREATE histograms when:**

1. **Uneven data distribution**
   - Example: 90% of orders from one country, 10% from others
   - Example: Most employees earn $30K-$50K, but a few earn $200K+

2. **Column used in WHERE clauses**
   - Example: `WHERE country = 'USA'`
   - Example: `WHERE salary > 100000`

3. **Queries are running slowly**
   - Oracle might be making wrong assumptions about your data

#### **👎 DON'T CREATE histograms when:**

1. **Even data distribution**
   - Example: Sales are roughly equal across all months

2. **Primary key columns**
   - Example: Customer ID, Order ID (these are unique anyway)

3. **Columns not used in WHERE clauses**
   - No point in creating histograms for columns you don't filter on

---

### 🎯 Practical Tips for Beginners

#### **Start Simple:**
1. Let Oracle handle histograms automatically (use `SIZE AUTO`)
2. Only intervene if you notice performance problems
3. Focus on columns that appear in your WHERE clauses

#### **Monitoring:**
```sql
-- Check which columns have histograms
SELECT 
    table_name,
    column_name,
    num_buckets,
    histogram
FROM USER_TAB_COL_STATISTICS 
WHERE histogram != 'NONE'
ORDER BY table_name, column_name;
```

#### **Common Beginner Mistakes to Avoid:**
- ❌ Creating histograms on every column
- ❌ Creating histograms on primary keys
- ❌ Ignoring histogram maintenance
- ✅ Let Oracle decide automatically
- ✅ Monitor query performance
- ✅ Only create histograms when needed

---

### 📚 Key Takeaways

1. **Histograms help Oracle make smarter decisions** about how to execute your queries
2. **Frequency histograms are more common** and easier to understand
3. **Let Oracle manage histograms automatically** unless you have specific performance issues
4. **Focus on columns used in WHERE clauses** with uneven data distribution
5. **Monitor your query performance** to see if histograms are helping

---

### 🔧 Quick Reference Commands

```sql
-- Check histogram settings
SELECT dbms_stats.get_prefs('METHOD_OPT') FROM dual;

-- View histograms on your tables
SELECT table_name, column_name, histogram, num_buckets 
FROM USER_TAB_COL_STATISTICS 
WHERE histogram != 'NONE';

-- Update table statistics (this may create histograms)
EXEC dbms_stats.gather_table_stats('YOUR_SCHEMA', 'YOUR_TABLE');
```

---

*Remember: As a beginner, it's better to let Oracle manage histograms automatically rather than trying to micromanage them. Focus on understanding your data and query patterns first!*

---
# Oracle Performance Tuning Guide - Chapters 9-11

## Table of Contents
- [Chapter 9: Performance Problem Analysis Methodology](#chapter-9-performance-problem-analysis-methodology)
- [Chapter 10: Dynamic Performance Views for Tuning](#chapter-10-dynamic-performance-views-for-tuning)
- [Chapter 11: SQL Tuning Health Check Script (SQLHC)](#chapter-11-sql-tuning-health-check-script-sqlhc)

---

## Chapter 9: Performance Problem Analysis Methodology

### Overview
This chapter provides a systematic approach to analyzing Oracle database performance problems. A well-planned methodology is crucial for successful performance tuning, as it ensures proper diagnosis before implementing any solutions. Without proper analysis, troubleshooting efforts can further delay resolution and potentially create additional issues.

### Key Principles of Performance Analysis

#### The Foundation Rule
Most common performance gains are achieved through SQL/Code/application tuning. Increasing hardware resources should always be your last resort. Database Administrators (DBAs) and Developers must work collaboratively when facing database/application performance issues, as developers play a crucial role in writing applications with effective SQL statements.

#### Initial Information Gathering
When performance issues are reported, the first step is to collect comprehensive information related to the problem. This preliminary investigation helps determine the appropriate troubleshooting approach.

### Preliminary Investigation Questions

Before diving into technical analysis, ask these fundamental questions to users and developers:

1. **What operations/program are executed?**
   - Identify the specific processes or applications experiencing issues

2. **Is it Oracle seeded or custom program?**
   - Determine if the issue involves standard Oracle functionality or custom-developed code

3. **How much time it used to take earlier?**
   - Establish baseline performance metrics for comparison

4. **Is the run time increased over time or you are seeing sudden increase in run time?**
   - Distinguish between gradual degradation and sudden performance drops

5. **Was there any recent code change/migration?**
   - Identify potential causes related to recent system changes

6. **Is it always slow or for certain time of the day only?**
   - Determine if the issue is consistent or time-dependent

7. **Is it slow for all parameters or for some specific parameters?**
   - Narrow down the scope of the performance problem

8. **How much data is getting processed?**
   - Understand the data volume impact on performance

9. **What is the frequency of job execution? Was there any change in frequency?**
   - Assess workload patterns and recent changes

10. **Does the problem happens on both their test and production systems?**
    - Determine environment-specific factors

### Performance Analysis Framework

Based on the preliminary information, you can decide which part of the system to target:

#### Target Categories
- **System-wide issues**: Address entire system performance
- **Session-specific issues**: Focus on individual database sessions
- **SQL statement issues**: Concentrate on specific query optimization

#### The Three Critical Questions

Your analysis should answer these fundamental questions:

##### 1. Where is time spent?
Identify the primary consumers of time in your system:
- Application code execution
- Network latency
- Disk I/O operations
- CPU processing
- Memory constraints

##### 2. How is time spent?
Determine time distribution across different layers:
- Database layer processing
- Application layer operations
- Network communication
- Disk subsystem operations
- Memory management

##### 3. How to reduce the time spent?
Based on the analysis, identify optimization opportunities:
- Focus on the major time consumers
- Implement targeted improvements
- Prioritize changes with highest impact

### System-Level Performance Analysis

When dealing with system-wide performance issues, follow these high-level steps:

#### System Resource Analysis
1. **CPU and Memory Assessment**
   - Use `vmstat`, `top`, or `prstat` to identify system-wide CPU/Memory consumption
   - Monitor for resource contention and bottlenecks

2. **Disk I/O Analysis**
   - Use `iostat` to verify if disks are the bottleneck
   - Check for high disk utilization and queue depths

3. **Network Performance**
   - Use `netstat` and `tnsping` to verify network connectivity and latency
   - Monitor for network-related delays

4. **System Environment Check**
   - Verify if other resource-intensive processes are running on the server
   - Check filesystem space availability
   - Review system logs for errors or warnings

#### Database-Specific Analysis
1. **Log Analysis**
   - Check Oracle alert logs for errors and warnings
   - Review application logs for performance-related messages
   - Analyze trace files for detailed information

2. **Locking Analysis**
   - Check for database locks that may cause contention
   - Identify blocking sessions and lock hierarchies

3. **AWR Report Generation**
   - Generate Automatic Workload Repository (AWR) reports
   - Analyze resource consumption patterns
   - Identify top SQL statements and wait events

4. **Memory and Storage Configuration**
   - Evaluate if increasing application/database memory will help
   - Check redo log configuration and sizing
   - Assess undo tablespace and temporary tablespace configurations
   - Review System Global Area (SGA) sizing and allocation

### Session-Level Performance Analysis

For issues specific to individual database sessions:

#### SQL Analysis
1. **Top SQL Identification**
   - Find the most resource-consuming SQL statements executing under the session
   - Prioritize optimization efforts based on resource consumption

2. **SQL Optimization Techniques**
   - Apply proven SQL optimization methodologies
   - Focus on the highest-impact statements first

3. **Session-Level Locking**
   - Verify locking at the individual session level
   - Identify blocking and waiting relationships

4. **Historical Analysis**
   - Generate AWR/ASH reports for the specific time duration
   - Collect useful information about session behavior patterns

### SQL-Level Performance Analysis

For problems with specific SQL statements:

#### Tracing and Analysis
1. **SQL Tracing**
   - Apply appropriate tracing mechanisms
   - Generate TKPROF files for detailed analysis
   - Focus on top resource-consuming SQL statements

2. **Execution Plan Analysis**
   - Avoid full table scans on large tables
   - Identify opportunities for index usage
   - Analyze join operations and access paths

#### Optimization Strategies
1. **Indexing Strategy**
   - Consider indexes on columns in WHERE clauses
   - Evaluate composite indexes for complex queries
   - Verify that existing indexes are valid and adequate

2. **AWR/ASH Analysis**
   - Use AWR/ASH reports to get collective performance information
   - Utilize tools like SQLTRPT and SQLT for detailed analysis

3. **Statistics and Metadata**
   - Verify that object statistics are current and accurate
   - Check for stale or missing statistics

4. **Advanced Optimization Techniques**
   - Evaluate parallelism opportunities
   - Consider materialized views for frequently accessed data
   - Implement SQL Plan Baselines for consistent performance
   - Utilize SQL Profiles for complex optimization scenarios

5. **Monitoring Long Operations**
   - Monitor V$SESSION_LONGOPS to detect long-running operations
   - Track progress of lengthy database operations

6. **Hint Usage**
   - Decide on using optimizer hints when they provide measurable improvement
   - Document hint usage and rationale

7. **Partitioning Strategy**
   - Consider table partitioning based on table characteristics and size
   - Evaluate partition pruning opportunities

### Best Practices for Performance Analysis

#### Systematic Approach
- Always follow a structured methodology
- Document findings and decisions
- Maintain a knowledge base of common issues and solutions

#### Collaboration
- Foster communication between DBAs and development teams
- Share performance analysis results and recommendations
- Establish regular performance review meetings

#### Continuous Monitoring
- Implement proactive monitoring strategies
- Set up alerts for performance threshold violations
- Regular review of system performance trends

---

## Chapter 10: Dynamic Performance Views for Tuning

### Overview
Oracle Database dynamic performance views are essential tools for querying important tuning-related parameters such as memory utilization, disk I/O, and disk structure information. These special views are continuously updated while a database is open and in use, providing real-time insights into database performance and behavior.

### Understanding Dynamic Performance Views

#### View Naming Conventions
- **V_$ Views**: The actual dynamic performance views identified by the prefix V_$
- **V$ Views**: Public synonyms for V_$ views with the prefix V$
- **Important**: Always access V$ objects, not the V_$ objects directly

#### RAC Environment Considerations
- **V$ vs GV$ Views**: For almost every V$ view, there is a corresponding GV$ (global V$) view in RAC databases
- **GV$ Functionality**: In RAC databases, querying a GV$ view retrieves V$ view information from all qualified instances
- **INST_ID Column**: Each GV$ view contains an extra column named INST_ID of datatype NUMBER, indicating the instance number from which the V$ view information was obtained

### Key Dynamic Performance Views

The following dynamic performance views are frequently used when troubleshooting performance issues:

#### V$OSSTAT - Operating System Statistics

**Purpose**: This view is particularly useful when you don't have direct access to the database server but want to monitor OS-level parameter values.

**Key Features**:
- The "Comments" column explains each parameter's meaning
- Parameters with "Cumulative" = "YES" show cumulative values since database instance startup
- Parameters with "Cumulative" = "NO" show current or constant values

**Usage Example**:
```sql
SELECT stat_name, value, comments 
FROM v$osstat;
```

**Common Statistics Available**:
- CPU utilization metrics
- Memory usage statistics
- I/O operation counts
- Network activity measures
- Process and thread information

#### V$EVENT_NAME - Wait Event Classification

**Purpose**: Oracle has numerous wait events (1200+) classified into 13 wait classes. This view helps identify which wait events belong to which wait classes.

**Key Information**:
- Complete list of all Oracle wait events
- Wait class categorization for each event
- Essential for understanding wait event significance

**Usage Example - Finding Idle Wait Events**:
```sql
SELECT * 
FROM v$event_name 
WHERE wait_class = 'Idle';
```

**Wait Classes Include**:
- Application
- Cluster
- Commit
- Concurrency
- Configuration
- Idle
- Network
- Other
- Scheduler
- System I/O
- User I/O
- Administrative
- Queueing

#### V$SESSION - Session Information and Activity

**Purpose**: V$SESSION is one of the most frequently used dynamic views, storing information about current database sessions and their activities.

**Critical Columns**:

##### Session Status and Type
- **STATUS**: Indicates if session is ACTIVE/INACTIVE/KILLED
- **TYPE**: Shows if session is for BACKGROUND process or USER process
- **STATE**: Shows if statement is currently waiting

##### User and Connection Information
- **USERNAME**: Database user for the connection
- **SCHEMANAME**: Database schema the session is connected to
- **OSUSER**: Operating system user who executed the session
- **LOGON_TIME**: When the session was initialized

##### SQL Activity Tracking
- **SQL_ID**: Current SQL statement being executed by the session
- **PREV_SQL_ID**: SQL_ID of the previously executed SQL statement
- **LAST_CALL_ET**: Time (in seconds) the session has been in current status

##### Performance Metrics
- **LOGICAL_READS**: Logical reads performed by the session
- **PHYSICAL_READS**: Physical reads performed by the session
- **BLOCK_GETS**: Number of times a current block was requested
- **CONSISTENT_GETS**: Number of times a consistent read was requested

**Usage Examples**:
```sql
-- Find active sessions and their current SQL
SELECT username, status, sql_id, last_call_et
FROM v$session
WHERE status = 'ACTIVE'
AND type = 'USER';

-- Identify long-running sessions
SELECT username, sql_id, last_call_et/3600 as hours_running
FROM v$session
WHERE last_call_et > 3600
ORDER BY last_call_et DESC;
```

#### V$PROCESS - Process Information

**Purpose**: Contains information about currently active Oracle processes, both background and user processes.

**Key Columns**:
- **SPID**: Operating system process identifier
- **USERNAME**: Operating system process username
- **BACKGROUND**: 1 for background process; NULL for normal process
- **PGA_USED_MEM**: PGA memory currently used by the process
- **PGA_MAX_MEM**: Maximum PGA memory ever allocated by the process

**Usage Example**:
```sql
SELECT spid, username, background, pga_used_mem, pga_max_mem
FROM v$process
WHERE pga_used_mem > 100000000; -- Processes using > 100MB PGA
```

#### V$SQL and V$SQLAREA - SQL Statement Information

**Purpose**: These views provide comprehensive information about SQL statements and their cursors.

**Key Differences**:
- **V$SQLAREA**: Parent-level cursor information for SQL statements
- **V$SQL**: Child-level cursor information for SQL statements

**Important Columns**:

##### SQL Identification and Text
- **SQL_ID**: Unique identifier for the SQL statement
- **SQL_TEXT**: First 1,000 characters of the SQL statement
- **SQL_FULLTEXT**: Complete SQL statement text
- **HASH_VALUE**: Hash value of the SQL statement

##### PL/SQL Information
- **PROGRAM_ID**: ID of the PL/SQL program if applicable
- **PROGRAM_LINE#**: Line number where SQL is located in PL/SQL

##### Parsing Information
- **LOADS**: Number of times hard parses occurred
- **FIRST_LOAD_TIME**: Timestamp of first hard parse
- **LAST_LOAD_TIME**: Timestamp of last hard parse

##### Optimizer Information
- **SQL_PROFILE**: Name of SQL profile used for execution plan generation
- **SQL_PLAN_BASELINE**: Name of SQL baseline used during execution plan generation
- **PLAN_HASH_VALUE**: Hash value of the execution plan linked to the SQL

##### Performance Metrics
- **ROWS_PROCESSED**: Number of rows processed by the SQL statement
- **ELAPSED_TIME**: Total database time used for processing (in microseconds)
- **CPU_TIME**: CPU time consumed by the SQL statement
- **DISK_READS**: Number of physical reads performed
- **BUFFER_GETS**: Number of logical reads performed
- **EXECUTIONS**: Number of times the SQL was executed

**Usage Examples**:
```sql
-- Find top SQL by elapsed time
SELECT sql_id, elapsed_time/1000000 as elapsed_seconds, executions,
       elapsed_time/executions/1000000 as avg_elapsed_seconds
FROM v$sql
WHERE executions > 0
ORDER BY elapsed_time DESC;

-- Find SQL with high logical reads
SELECT sql_id, buffer_gets, executions,
       buffer_gets/executions as avg_buffer_gets
FROM v$sql
WHERE executions > 0
AND buffer_gets/executions > 10000
ORDER BY buffer_gets DESC;
```

### Advanced Usage Patterns

#### Combining Views for Comprehensive Analysis
```sql
-- Session with current SQL details
SELECT s.username, s.status, s.sql_id, sq.sql_text,
       s.last_call_et, sq.elapsed_time/1000000 as sql_elapsed_seconds
FROM v$session s, v$sql sq
WHERE s.sql_id = sq.sql_id
AND s.status = 'ACTIVE'
AND s.type = 'USER';
```

#### Historical Analysis Considerations
- V$ views show current state information
- For historical analysis, use AWR and ASH data
- Combine current V$ view data with historical reports for comprehensive analysis

#### Performance Monitoring Queries
```sql
-- System-wide wait events
SELECT event, total_waits, time_waited/100 as time_waited_seconds
FROM v$system_event
WHERE wait_class != 'Idle'
ORDER BY time_waited DESC;

-- Session wait events
SELECT s.username, se.event, se.total_waits, se.time_waited/100 as time_waited_seconds
FROM v$session s, v$session_event se
WHERE s.sid = se.sid
AND s.username IS NOT NULL
AND se.wait_class != 'Idle'
ORDER BY se.time_waited DESC;
```

### Best Practices for Using Dynamic Performance Views

#### Query Efficiency
- Use appropriate WHERE clauses to limit result sets
- Avoid querying V$ views too frequently in production
- Consider the overhead of complex queries on performance

#### Security Considerations
- V$ views require appropriate privileges to access
- Consider granting specific SELECT privileges rather than broad access
- Monitor who has access to sensitive performance data

#### Integration with Other Tools
- Use V$ views data to supplement AWR/ASH analysis
- Combine with OS-level monitoring for complete picture
- Integrate with custom monitoring solutions

---

## Chapter 11: SQL Tuning Health Check Script (SQLHC)

### Overview
The SQL Tuning Health Check Script (SQLHC) is a comprehensive diagnostic tool designed to analyze poorly performing SQL statements in Oracle databases. Unlike other tuning tools that require installation, SQLHC is a standalone script that focuses on specific SQL statements, examining Cost-based Optimizer (CBO) statistics, schema object metadata, configuration parameters, and other elements that influence SQL performance.

### Key Features and Advantages

#### Standalone Operation
- **No Installation Required**: SQLHC doesn't require any code to be installed in the database
- **Non-Intrusive**: Uses existing SQL_ID of statements already executed and present in memory
- **No SQL Re-execution**: Generates reports without re-executing the target SQL statement

#### Comprehensive Analysis
SQLHC provides extensive information about SQL performance factors:
- Execution plan analysis and historical plan changes
- Statistics validity assessment with explanations
- Table and index detailed information
- Object statistics analysis
- SQL text, profiles, and baseline details
- Historical execution plan details
- Dynamic performance view insights

#### Version and Environment Support
- **Oracle Version**: Works with Oracle 10g and above
- **RAC Awareness**: Fully compatible with Oracle Real Application Clusters (RAC)
- **License Flexibility**: Works with different Oracle license combinations

### Information Provided by SQLHC

#### Execution Plan Analysis
- **Current Execution Plans**: Detailed analysis of current SQL execution plans
- **Plan Changes**: Historical tracking of execution plan modifications
- **Plan Stability**: Assessment of plan consistency over time

#### Statistics and Metadata Validation
- **Object Statistics**: Comprehensive review of table and index statistics
- **Statistics Freshness**: Validation of statistics currency and accuracy
- **Parameter Analysis**: Configuration parameter evaluation with explanations

#### Schema Object Information
- **Table Details**: Complete table metadata and characteristics
- **Index Information**: Index structure, usage, and effectiveness analysis
- **Constraint Analysis**: Foreign key, primary key, and check constraint impact

#### SQL Performance Metrics
- **Execution History**: Historical performance data and trends
- **Resource Consumption**: CPU, I/O, and memory usage patterns
- **Wait Events**: SQL-specific wait event analysis

#### Advanced Features
- **SQL Profiles**: Analysis of existing SQL profiles and their effectiveness
- **Baselines**: SQL Plan Baseline usage and management recommendations
- **Dynamic Performance Views**: Relevant V$ view data compilation

### Obtaining and Installing SQLHC

#### Download Source
SQLHC is available through Oracle My Oracle Support (MOS):
- **Document ID**: 1366133.1
- **Title**: "SQL Tuning Health-Check Script (SQLHC)"
- **Access**: Requires valid Oracle Support account

#### Installation Process
1. **Download**: Access MOS Note 1366133.1 and download the SQLHC zip file
2. **Extract**: Unzip the downloaded file to your preferred directory
3. **Upload**: Transfer the script files to your database server

**File Structure After Extraction**:
```
sqlhc/
├── sqlhc.sql      (Main script file)
├── sqldx.sql      (Diagnostics component)
└── sqlhcxec.sql   (Execution component)
```

### Execution Requirements and Parameters

#### Prerequisites
- **Database Access**: Connect as SYS, DBA, or user with Data Dictionary view access
- **SQL_ID Identification**: Target SQL statement must be identified and in memory
- **License Information**: Knowledge of Oracle pack licensing

#### Required Parameters

##### 1. Oracle Pack License Parameter
**Format**: [T|D|N]
- **T**: Tuning Pack (includes Diagnostics Pack functionality)
- **D**: Diagnostics Pack only
- **N**: No additional packs licensed

**Important**: If both Tuning and Diagnostics licenses are available, specify 'T' as Tuning Pack includes Diagnostics Pack capabilities.

##### 2. SQL_ID Parameter
- **Format**: Valid SQL_ID (typically alphanumeric, 13 characters)
- **Source**: Can be obtained from AWR reports, ASH reports, or V$SQL views
- **Requirement**: SQL statement must have been executed and be present in memory

### Step-by-Step Execution Guide

#### 1. Preparation Phase
```bash
# Extract the downloaded file
$ unzip sqlhc.zip
Archive: sqlhc.zip
  creating: sqlhc/
  inflating: sqlhc/sqlhc.sql
  inflating: sqlhc/sqldx.sql
  inflating: sqlhc/sqlhcxec.sql

# Navigate to the script directory
$ cd sqlhc

# Verify file contents
$ ls -tlr
total 2
-rw-r--r-- 1 oracle dba  48747 Nov 11 2013 sqldx.sql
-rw-r--r-- 1 oracle dba 288298 Apr 16 2014 sqlhc.sql
-rw-r--r-- 1 oracle dba 292838 Apr 16 2014 sqlhcxec.sql
```

#### 2. SQL_ID Identification
Before running SQLHC, identify the problematic SQL statement:

```sql
-- Find SQL_ID from V$SQL based on SQL text pattern
SELECT sql_id, sql_text, executions, elapsed_time
FROM v$sql
WHERE UPPER(sql_text) LIKE '%YOUR_TABLE_NAME%'
AND sql_text NOT LIKE '%sql_text%'; -- Exclude this query itself

-- Find top resource-consuming SQL statements
SELECT sql_id, elapsed_time/1000000 as elapsed_seconds, 
       cpu_time/1000000 as cpu_seconds, executions
FROM v$sql
WHERE executions > 0
ORDER BY elapsed_time DESC;
```

#### 3. Script Execution
```sql
-- Connect to database with appropriate privileges
$ sqlplus / as sysdba

-- Execute SQLHC with parameters
SQL> @sqlhc.sql T 1w6s3ayu0kw8m
```

**Parameter Explanation**:
- `T`: Indicates Tuning pack license (includes Diagnostics)
- `1w6s3ayu0kw8m`: Example SQL_ID of the statement to analyze

#### 4. Script Processing
During execution, SQLHC will:
- Validate the provided SQL_ID
- Check licensing parameters
- Collect comprehensive performance data
- Generate HTML report with analysis
- Create downloadable ZIP file

### Output Analysis and Interpretation

#### Generated Files
SQLHC creates a ZIP file containing:
- **HTML Report**: Comprehensive analysis in web-readable format
- **Supporting Files**: Raw data and detailed diagnostics
- **Summary Information**: Key findings and recommendations

#### Report Sections

##### Executive Summary
- **Performance Overview**: High-level performance assessment
- **Critical Issues**: Major problems identified
- **Recommendations**: Prioritized improvement suggestions

##### Detailed Analysis Sections

###### 1. SQL Statement Information
- Complete SQL text and formatting
- Bind variable usage analysis
- Parsing and sharing cursor information

###### 2. Execution Plan Analysis
- Current execution plans with costs
- Historical plan variations
- Plan stability assessment
- Optimizer decision analysis

###### 3. Object Statistics Review
- Table statistics freshness and accuracy
- Index statistics validation
- Histogram analysis where applicable
- Statistics collection recommendations

###### 4. Performance Metrics
- Resource consumption patterns
- Wait event analysis
- Execution frequency and timing
- Performance trend analysis

###### 5. Configuration Assessment
- Relevant initialization parameters
- Optimizer settings impact
- Memory allocation analysis
- I/O configuration review

### Best Practices for SQLHC Usage

#### Pre-Execution Planning
1. **Performance Issue Documentation**: Document specific performance symptoms
2. **Baseline Establishment**: Gather current performance metrics
3. **Environment Assessment**: Understand system load during analysis

#### Effective Analysis Approach
1. **Focus on Critical Issues**: Prioritize high-impact findings
2. **Correlate with System Metrics**: Compare with AWR/ASH data
3. **Historical Context**: Consider performance trends over time

#### Implementation Guidelines
1. **Test Environment First**: Validate recommendations in test systems
2. **Incremental Changes**: Implement one change at a time
3. **Performance Monitoring**: Monitor impact of each modification

### Integration with Other Tools

#### Complementary Analysis Tools
- **AWR Reports**: System-wide performance context
- **ASH Reports**: Session-level activity analysis
- **SQL Tuning Advisor**: Oracle's automated tuning recommendations
- **SQL Plan Management**: Execution plan stability tools

#### Workflow Integration
1. **Initial Analysis**: Use AWR/ASH to identify problematic SQL
2. **Detailed Assessment**: Apply SQLHC for specific SQL analysis
3. **Tuning Implementation**: Apply recommendations systematically
4. **Performance Validation**: Monitor improvements using AWR/ASH

### Troubleshooting Common Issues

#### Access and Permission Problems
- Verify database connection privileges
- Ensure Data Dictionary view access
- Check Oracle Support account validity

#### SQL_ID Related Issues
- Confirm SQL_ID exists in V$SQL
- Verify SQL statement was recently executed
- Check for SQL_ID case sensitivity

#### Licensing Considerations
- Understand your Oracle license agreements
- Choose appropriate licensing parameter
- Document licensing decisions for audit purposes

### Advanced Usage Scenarios

#### RAC Environment Analysis
- Execute on each RAC node for complete analysis
- Compare cross-instance execution patterns
- Analyze interconnect-related performance impacts

#### Historical Performance Investigation
- Combine with AWR historical data
- Track SQL performance degradation over time
- Identify correlation with system changes

#### Development and Testing Integration
- Include in code review processes
- Automate analysis for critical SQL statements
- Establish performance baseline standards

---

### Key Takeaways

1. **Methodology First**: Always follow a structured approach to performance analysis
2. **Comprehensive Data Collection**: Use multiple tools and views for complete analysis
3. **Collaborative Approach**: Engage both DBAs and developers in performance optimization
4. **Focus on High Impact**: Prioritize SQL and application tuning over hardware upgrades
5. **Continuous Monitoring**: Implement ongoing performance monitoring and analysis

### Next Steps

- Practice using these methodologies in your environment
- Establish performance baselines for critical systems
- Create standardized procedures for performance issue resolution
- Build a knowledge base of common performance patterns and solutions
- Implement proactive monitoring using the tools and techniques described
