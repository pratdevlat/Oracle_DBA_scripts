# PostgreSQL Architecture Deep Dive

## Overview

PostgreSQL uses a multi-process architecture built around a central postmaster process that manages client connections and spawns various background processes to handle different database operations. This architecture provides excellent concurrency, crash safety, and ACID compliance through careful coordination between memory structures, processes, and logging mechanisms.

## Process Architecture

### Postmaster Process

The **postmaster** serves as the main supervisor process that:

- Listens for incoming client connections
- Forks backend processes to handle each client session
- Manages and monitors all background processes
- Handles system shutdown and startup procedures

### Backend Processes

Each client connection gets a dedicated **backend process** that:

- Persists for the entire session duration
- Handles all SQL queries from that specific client
- Manages local memory and temporary files
- Communicates with shared memory and background processes

### Background Processes

**Background Writer**

- Continuously flushes dirty pages from shared buffers to disk
- Reduces I/O burden during checkpoints
- Helps maintain steady write performance

**Checkpointer**

- Performs periodic checkpoints at configured intervals
- Ensures all dirty data is written to persistent storage
- Creates consistent recovery points for crash recovery
- Updates control file with checkpoint information

**WAL Writer**

- Flushes write-ahead log records from memory buffers to disk
- Operates on timer-based intervals or when WAL buffers fill
- Critical for transaction durability guarantees

**Autovacuum Launcher & Workers**

- Launcher spawns worker processes based on table activity
- Workers reclaim storage from dead tuples
- Update table statistics for query optimization
- Prevent transaction ID wraparound

**Statistics Collector**

- Gathers performance and usage statistics
- Tracks table access patterns, index usage, and query performance
- Provides data for pg_stat_* system views

## Memory Architecture

### Shared Memory Components

**shared_buffers**

- Main buffer pool shared among all backend processes
- Caches frequently accessed data pages in memory
- Typically configured to 25-40% of available system RAM
- Uses clock-sweep algorithm for page replacement
- Tracks page usage counts to optimize replacement decisions

**WAL Buffers (wal_buffers)**

- Holds WAL records in memory before disk writes
- Shared among all processes generating WAL
- Flushed by WAL writer process or when full
- Size typically auto-tuned based on shared_buffers

**Lock Tables**

- Manages all database locks (table, row, advisory)
- Tracks lock holders and waiters
- Enables deadlock detection algorithms

### Process-Local Memory

**work_mem**

- Controls memory allocation for query operations within each backend
- Used for operations like:
  - Sorting (ORDER BY, CREATE INDEX)
  - Hash joins and hash aggregations
  - Bitmap index scans
  - Recursive queries (CTEs)
- Multiple operations in single query can each use up to work_mem
- Critical tuning parameter for query performance vs. memory usage

**maintenance_work_mem**

- Governs memory for maintenance operations:
  - VACUUM operations
  - CREATE INDEX, REINDEX
  - ALTER TABLE operations
  - Foreign key constraint checking
- Typically set much higher than work_mem
- Used less frequently but benefits significantly from more memory

**temp_buffers**

- Local buffer pool for temporary tables
- Separate from shared_buffers
- Per-session allocation

## Multi-Version Concurrency Control (MVCC)

### Transaction Snapshots

Each transaction receives a snapshot containing:

- List of active transactions at snapshot time
- Highest committed transaction ID
- Visibility rules for determining which tuple versions to see

### Tuple Versioning

Every row (tuple) contains system columns:

**xmin (Transaction ID)**

- Records which transaction inserted this tuple version
- Used to determine when tuple became visible

**xmax (Transaction ID)**

- Records which transaction deleted/updated this tuple
- 0 if tuple is still current/active
- Non-zero indicates tuple is deleted or superseded

**ctid (Item Pointer)**

- Physical location of tuple on disk
- Used for tuple chaining in updates

### Visibility Rules

A tuple version is visible to a transaction if:

1. The inserting transaction (xmin) committed before the reading transactionâ€™s snapshot
1. The deleting transaction (xmax) is either:
- 0 (tuple not deleted), OR
- Had not committed when the snapshot was taken

### MVCC Benefits and Trade-offs

**Benefits:**

- Readers never block writers
- Writers never block readers (except for explicit locks)
- Consistent read views without locking
- High concurrency for mixed workloads

**Trade-offs:**

- Creates dead tuples requiring cleanup (VACUUM)
- Increased storage overhead for multiple versions
- More complex visibility checking logic

## Write-Ahead Logging (WAL)

### WAL Fundamentals

WAL ensures durability through the principle: **log changes before applying them to data files**

Every database modification generates WAL records containing:

- Transaction ID and timestamp
- Type of operation (INSERT, UPDATE, DELETE, etc.)
- Before and after images of changed data
- Sufficient information to redo or undo the change

### WAL Record Structure

```
WAL Record = Header + Data
Header contains:
- Record length and CRC checksum
- Transaction ID and timestamp  
- Resource manager ID (table, index, etc.)
- Record type and flags
```

### WAL Writing Process

1. Backend processes generate WAL records in memory
1. Records are written to WAL buffers (wal_buffers)
1. WAL writer process flushes buffers to disk files
1. Group commit optimization batches multiple transactions
1. fsync() ensures data reaches persistent storage

### WAL Files Management

- WAL files stored in pg_wal directory
- Files are typically 16MB each (configurable)
- Files are recycled/renamed rather than deleted
- Archive mode can copy completed WAL files for backup

### Checkpoints and Recovery

**Checkpoint Process:**

1. Force all dirty buffers to disk
1. Update control file with checkpoint location
1. Allows old WAL files to be recycled
1. Creates known good recovery starting point

**Crash Recovery:**

1. Start from last valid checkpoint
1. Replay WAL records forward to reconstruct database state
1. Apply both committed and uncommitted changes
1. Use transaction status to determine final visibility

### WAL Applications

- **Crash Recovery:** Restore database after unexpected shutdown
- **Point-in-Time Recovery:** Restore to specific moment using base backup + WAL
- **Streaming Replication:** Send WAL records to standby servers
- **Logical Replication:** Extract logical changes for replication

## Query Processing Flow

### Query Lifecycle

1. **Connection:** Client connects, postmaster forks backend process
1. **Parsing:** SQL text parsed into parse tree, syntax validation
1. **Analysis:** Semantic analysis, name resolution, type checking
1. **Planning:** Cost-based optimizer creates execution plan
1. **Execution:** Plan executed, may use various memory areas
1. **Results:** Data returned to client, resources cleaned up

### Memory Usage During Execution

- **Parsing/Planning:** Uses relatively small amounts of local memory
- **Sorting:** May use up to work_mem per sort operation
- **Hash Operations:** Hash tables limited by work_mem
- **Buffer Access:** Data pages accessed through shared_buffers
- **WAL Generation:** Modifications create WAL records in shared WAL buffers

## Performance Tuning Considerations

### Memory Configuration Guidelines

- **shared_buffers:** Start with 25% of RAM, tune based on workload
- **work_mem:** Conservative starting point (4-8MB), increase for complex queries
- **maintenance_work_mem:** 256MB to 1GB depending on maintenance operations
- **effective_cache_size:** Set to ~75% of total system RAM

### Monitoring Key Metrics

- Buffer hit ratio (aim for >95%)
- WAL generation rate and checkpoint frequency
- Lock waits and deadlocks
- Vacuum and autovacuum effectiveness
- Query execution plans and timing

This architecture provides PostgreSQL with its renowned reliability, performance, and feature richness while maintaining full ACID compliance and supporting high-concurrency workloads.